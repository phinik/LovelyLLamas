{
    "activation": "relu",
    "d_model": 64,
    "dim_feedforward": 256,
    "dropout": 0.1,
    "n_head": 4,
    "n_layers": 2,
    "n_position": 200
}