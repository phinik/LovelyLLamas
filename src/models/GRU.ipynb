{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "# Special tokens for text substitution\n",
    "special_tokens = {\n",
    "    'additional_special_tokens': ['<city>','<temp>','<date>','<velocity>','<percentile>','<rainfall>']\n",
    "}\n",
    "\n",
    "# Add special tokens into tokenizer\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, weather_data, max_length=100):\n",
    "        self.data = [weather_data] if isinstance(weather_data, dict) else weather_data\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize wind directions from all data points\n",
    "        self.wind_directions = sorted(list(set([d for data in self.data for d in data['windrichtung']])))\n",
    "        self.wind_dir_to_idx = {d: i for i, d in enumerate(self.wind_directions)}\n",
    "        \n",
    "        # Calculate expected feature dimension\n",
    "        self.feature_dim = (\n",
    "            1 +  # temperature\n",
    "            1 +  # rain risk\n",
    "            1 +  # rain amount\n",
    "            1 +  # wind speed\n",
    "            1 +  # pressure\n",
    "            1 +  # humidity\n",
    "            1 +  # cloudiness\n",
    "            len(self.wind_directions) +  # one-hot wind directions\n",
    "            2 +  # time encoding (sin, cos)\n",
    "            3 +  # sun features\n",
    "            1    # sun hours\n",
    "        )\n",
    "        \n",
    "    def replace_dates(self, text: str) -> str:\n",
    "        text = re.sub(r\"\\b\\d{1,2}\\.\\d{1,2}\\.\\d{4}\\b\", \"<date>\", text)\n",
    "        return text\n",
    "    \n",
    "    def replace_city_and_units(self, text: str, city: str) -> str:\n",
    "        text = re.sub(city, '<city>', text)\n",
    "        unit_patterns = [\n",
    "        # TEMPERATURE\n",
    "        (r'(°[ ]*C|Grad)', ' <temp>'),\n",
    "        # VELOCITY\n",
    "        (r'[ ]*km/h', ' <velocity>'),\n",
    "        # PERCENTILE\n",
    "        (r'[ ]*%', ' <percentile>'),\n",
    "        # RAINFALL DELTA\n",
    "        # (r'\\d+\\.\\d+ bis \\d+\\.\\d+[ ]*l\\/m²', '<rainfall> bis <rainfall>'),\n",
    "        (r'[ ]*l\\/m²', ' <rainfall>')\n",
    "        ]\n",
    "        for pattern, replacement in unit_patterns:\n",
    "            try:\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        # REMOVE MARKUP\n",
    "        text = re.sub(r'\\**', '', text)\n",
    "\n",
    "        # REMOVE WEIRD PUNCUATION\n",
    "        text = re.sub(r' \\.', '.', text)\n",
    "\n",
    "        # REMOVE UNNECESSARY NEWLINES\n",
    "        text = re.sub(r'\\n\\n', '\\n', text)\n",
    "\n",
    "        # REMOVE SPACE AFTER NEWLINE\n",
    "        text = re.sub(r'\\n ', '\\n', text)\n",
    "\n",
    "        # REPLACE MULTIPLE WHITESPACES WITH ONE\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def _parse_time(self, time_str):\n",
    "        \"\"\"Parse time string and handle missing data\"\"\"\n",
    "        if time_str == '-' or not time_str:\n",
    "            return None\n",
    "        try:\n",
    "            # Handle \"HH:MM Uhr\" format\n",
    "            if ':' in time_str:\n",
    "                hour, minute = map(int, time_str.split(' ')[0].split(':'))\n",
    "                return hour + minute/60\n",
    "            return None\n",
    "        except (ValueError, IndexError):\n",
    "            return None\n",
    "\n",
    "    def _encode_time(self, time_str):\n",
    "        # Convert \"HH - HH Uhr\" to cyclic features\n",
    "        try:\n",
    "            start_hour = int(time_str.split(' - ')[0])\n",
    "            hour_sin = torch.sin(torch.tensor(2 * math.pi * start_hour / 24))\n",
    "            hour_cos = torch.cos(torch.tensor(2 * math.pi * start_hour / 24))\n",
    "            return torch.tensor([hour_sin, hour_cos])\n",
    "        except (ValueError, IndexError):\n",
    "            # Return neutral values for invalid time\n",
    "            return torch.tensor([0.0, 1.0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _encode_sun_info(self, sunrise, sunset, current_time):\n",
    "        # Parse times, handling missing data\n",
    "        sunrise_hour = self._parse_time(sunrise)\n",
    "        sunset_hour = self._parse_time(sunset)\n",
    "        \n",
    "        try:\n",
    "            current_hour = float(current_time.split(' - ')[0])\n",
    "        except (ValueError, IndexError):\n",
    "            # Return default values if current time is invalid\n",
    "            return torch.tensor([0.0, 0.0, 0.0])\n",
    "        \n",
    "        # If sunrise or sunset is missing, use approximate values based on season\n",
    "        if sunrise_hour is None or sunset_hour is None:\n",
    "            # Return default encoding indicating uncertainty\n",
    "            return torch.tensor([\n",
    "                0.5,  # Unknown daylight status\n",
    "                0.0,  # Neutral time since sunrise\n",
    "                0.0   # Neutral time until sunset\n",
    "            ])\n",
    "        \n",
    "        # Calculate daylight features\n",
    "        is_daylight = (current_hour >= sunrise_hour) and (current_hour <= sunset_hour)\n",
    "        \n",
    "        if is_daylight:\n",
    "            time_since_sunrise = (current_hour - sunrise_hour) / (sunset_hour - sunrise_hour)\n",
    "            time_until_sunset = (sunset_hour - current_hour) / (sunset_hour - sunrise_hour)\n",
    "        else:\n",
    "            if current_hour < sunrise_hour:\n",
    "                time_since_sunrise = -1 * (sunrise_hour - current_hour) / (24 - sunset_hour + sunrise_hour)\n",
    "                time_until_sunset = -1\n",
    "            else:\n",
    "                time_since_sunrise = -1\n",
    "                time_until_sunset = -1 * (current_hour - sunset_hour) / (24 - sunset_hour + sunrise_hour)\n",
    "        \n",
    "        return torch.tensor([float(is_daylight), time_since_sunrise, time_until_sunset])\n",
    "\n",
    "    def one_hot_wind(self, wind_dir):\n",
    "        encoding = torch.zeros(len(self.wind_directions))\n",
    "        encoding[self.wind_dir_to_idx[wind_dir]] = 1\n",
    "        return encoding\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Get sequence length from the data\n",
    "        seq_len = len(item['temperatur_in_deg_C'])\n",
    "        \n",
    "        # Initialize features tensor with correct shape\n",
    "        features = torch.zeros((seq_len, self.feature_dim))\n",
    "        \n",
    "        # Fill features one by one, maintaining consistent shapes\n",
    "        current_idx = 0\n",
    "        \n",
    "        # Numerical features - all should be shape [seq_len, 1]\n",
    "        features[:, current_idx] = torch.tensor([float(t) for t in item['temperatur_in_deg_C']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        features[:, current_idx] = torch.tensor([float(r) for r in item['niederschlagsrisiko_in_perc']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        # Handle rain amount with forward filling for NaN values\n",
    "        rain_values = []\n",
    "        last_valid = 0.0\n",
    "        for r in item['niederschlagsmenge_in_l_per_sqm']:\n",
    "            try:\n",
    "                val = float(r)\n",
    "                if not torch.isnan(torch.tensor(val)):\n",
    "                    last_valid = val\n",
    "                rain_values.append(last_valid)\n",
    "            except ValueError:\n",
    "                rain_values.append(last_valid)\n",
    "        features[:, current_idx] = torch.tensor(rain_values)\n",
    "        current_idx += 1\n",
    "        \n",
    "        features[:, current_idx] = torch.tensor([float(w) for w in item['windgeschwindigkeit_in_km_per_s']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        features[:, current_idx] = torch.tensor([float(p) for p in item['luftdruck_in_hpa']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        features[:, current_idx] = torch.tensor([float(h) for h in item['relative_feuchte_in_perc']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        features[:, current_idx] = torch.tensor([float(c.split('/')[0]) / 8 for c in item['bewölkungsgrad']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        # Wind directions (one-hot encoded)\n",
    "        wind_features = torch.stack([self.one_hot_wind(w) for w in item['windrichtung']])\n",
    "        features[:, current_idx:current_idx + len(self.wind_directions)] = wind_features\n",
    "        current_idx += len(self.wind_directions)\n",
    "        \n",
    "        # Time features\n",
    "        time_features = torch.stack([self._encode_time(t) for t in item['times']])\n",
    "        features[:, current_idx:current_idx + 2] = time_features\n",
    "        current_idx += 2\n",
    "        \n",
    "        # Sun features\n",
    "        sun_features = torch.stack([\n",
    "            self._encode_sun_info(\n",
    "                item.get('sunrise', '-'), \n",
    "                item.get('sundown', '-'), \n",
    "                t\n",
    "            ) for t in item['times']\n",
    "        ])\n",
    "        features[:, current_idx:current_idx + 3] = sun_features\n",
    "        current_idx += 3\n",
    "        \n",
    "        # Sun hours feature\n",
    "        sun_hours = torch.tensor([1.0 if \"fast nicht zu sehen\" in item.get('sunhours', '') else 0.0])\n",
    "        features[:, current_idx] = sun_hours.expand(seq_len)\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'text': self.replace_dates(self.replace_city_and_units(item['report_long'], item['city']))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0072727555af4d8cafae5061d0af3688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    # change directory if not on root\n",
    "    if str(os.getcwd()).endswith('LLamas') == False:\n",
    "        os.chdir('../..')\n",
    "    \n",
    "    # check if dict contains correct key\n",
    "    # def check_file(path):\n",
    "    #     with open(path, 'r', encoding='utf-8') as f:\n",
    "    #         data: dict = json.load(f)\n",
    "    #         if 'gpt_rewritten_v2' in data.keys():\n",
    "    #             return True\n",
    "    #         else:\n",
    "    #             return False\n",
    "\n",
    "    def check_file(path):\n",
    "        \"\"\"\n",
    "        Checks if the file contains valid keys and values.\n",
    "        Returns True if the file should be loaded, False otherwise.\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data: dict = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Invalid JSON in file: {path}\")\n",
    "                return False\n",
    "\n",
    "            # Ensure required keys are present\n",
    "            if not {'report_long', 'city'}.issubset(data.keys()):\n",
    "                return False\n",
    "\n",
    "            # Check if 'city' and 'report_long' have valid non-empty values\n",
    "            if not isinstance(data['city'], str) or not data['city'].strip():\n",
    "                return False\n",
    "            if not isinstance(data['report_long'], str) or not data['report_long'].strip():\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "                \n",
    "    # transform data into weatherDataSet class format\n",
    "    def load_data(path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data: dict = json.load(f)\n",
    "            return data\n",
    "    \n",
    "    # files for reading\n",
    "    files = os.listdir(os.path.join(os.getcwd(), 'data', 'files_for_chatGPT', '2024-12-12'))\n",
    "    files = {(file.split('-')[-1]).split('_')[0]:load_data(os.path.join(os.getcwd(), 'data', 'files_for_chatGPT', '2024-12-12', file)) for file in tqdm(files) if check_file(os.path.join(os.getcwd(), 'data', 'files_for_chatGPT', '2024-12-12', file))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Summary:\n",
      "Total samples: 28811\n",
      "Samples with issues: 43\n",
      "Total NaN values: 454\n",
      "Total infinite values: 0\n",
      "Total extreme values: 0\n",
      "\n",
      "Removed 43 samples\n",
      "Remaining samples: 28768\n"
     ]
    }
   ],
   "source": [
    "weather_data = list(files.values())\n",
    "dataset = WeatherDataset(weather_data, max_length=100)\n",
    "def validate_and_clean_weather_data(weather_data, dataset_class):\n",
    "    \"\"\"\n",
    "    Validates the dataset and returns cleaned weather_data with problematic samples removed.\n",
    "    \n",
    "    Args:\n",
    "        weather_data: List of weather data samples\n",
    "        dataset_class: The dataset class constructor to use for validation\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (cleaned_weather_data, removed_indices, validation_summary)\n",
    "    \"\"\"\n",
    "    # Create temporary dataset for validation\n",
    "    temp_dataset = dataset_class(weather_data, max_length=100)\n",
    "    \n",
    "    summary = {\n",
    "        'total_samples': len(temp_dataset),\n",
    "        'invalid_samples': [],\n",
    "        'statistics': {\n",
    "            'nan_count': 0,\n",
    "            'inf_count': 0,\n",
    "            'extreme_values': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    invalid_indices = set()\n",
    "    \n",
    "    # Validate each sample\n",
    "    for idx in range(len(temp_dataset)):\n",
    "        try:\n",
    "            sample = temp_dataset[idx]\n",
    "            features = sample['features']\n",
    "            \n",
    "            has_issue = False\n",
    "            \n",
    "            # Check for NaN values\n",
    "            nan_mask = torch.isnan(features)\n",
    "            if nan_mask.any():\n",
    "                summary['statistics']['nan_count'] += nan_mask.sum().item()\n",
    "                has_issue = True\n",
    "                \n",
    "            # Check for infinity\n",
    "            inf_mask = torch.isinf(features)\n",
    "            if inf_mask.any():\n",
    "                summary['statistics']['inf_count'] += inf_mask.sum().item()\n",
    "                has_issue = True\n",
    "                \n",
    "            # Check for extreme values\n",
    "            extreme_mask = (features.abs() > 1e6)\n",
    "            if extreme_mask.any():\n",
    "                summary['statistics']['extreme_values'] += extreme_mask.sum().item()\n",
    "                has_issue = True\n",
    "            \n",
    "            if has_issue:\n",
    "                invalid_indices.add(idx)\n",
    "                summary['invalid_samples'].append({\n",
    "                    'index': idx,\n",
    "                    'text': sample['text']\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "            invalid_indices.add(idx)\n",
    "            summary['invalid_samples'].append({\n",
    "                'index': idx,\n",
    "                'error_message': str(e)\n",
    "            })\n",
    "    \n",
    "    # Create cleaned weather_data list\n",
    "    cleaned_weather_data = [\n",
    "        data for idx, data in enumerate(weather_data) \n",
    "        if idx not in invalid_indices\n",
    "    ]\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nValidation Summary:\")\n",
    "    print(f\"Total samples: {summary['total_samples']}\")\n",
    "    print(f\"Samples with issues: {len(summary['invalid_samples'])}\")\n",
    "    print(f\"Total NaN values: {summary['statistics']['nan_count']}\")\n",
    "    print(f\"Total infinite values: {summary['statistics']['inf_count']}\")\n",
    "    print(f\"Total extreme values: {summary['statistics']['extreme_values']}\")\n",
    "    print(f\"\\nRemoved {len(invalid_indices)} samples\")\n",
    "    print(f\"Remaining samples: {len(cleaned_weather_data)}\")\n",
    "    \n",
    "    return cleaned_weather_data, list(invalid_indices), summary\n",
    "\n",
    "# Example usage function\n",
    "def clean_and_create_dataset(weather_data, dataset_class):\n",
    "    \"\"\"\n",
    "    Cleans the weather data and creates a new dataset.\n",
    "    \n",
    "    Args:\n",
    "        weather_data: Original weather data list\n",
    "        dataset_class: Dataset class to use\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (new_dataset, removed_indices, validation_summary)\n",
    "    \"\"\"\n",
    "    cleaned_data, removed_indices, summary = validate_and_clean_weather_data(\n",
    "        weather_data, dataset_class\n",
    "    )\n",
    "    \n",
    "    # Create new dataset with cleaned data\n",
    "    clean_dataset = dataset_class(cleaned_data, max_length=100)\n",
    "    \n",
    "    return clean_dataset, removed_indices, summary\n",
    "\n",
    "# Clean the data and create new dataset\n",
    "clean_dataset, removed_indices, summary = clean_and_create_dataset(weather_data, WeatherDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOMALY DETECTION RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original texts for test samples:\n",
      "Sample 0: Wetter heute, <date> In <city> stören am Morgen nur einzelne Wolken den sonst blauen Himmel bei Temperaturen von 23 <temp>. Mittags stören nur einzelne Wolken den sonst blauen Himmel und die Temperaturen erreichen 30 <temp>. Abends gibt es in <city> lockere Bewölkung bei Temperaturen von 24 bis 26 <temp>. In der Nacht bedecken einzelne Wolken den Himmel und die Werte gehen auf 22 <temp> zurück. Die gefühlten Temperaturen liegen bei 23 bis 33 <temp>. <city> liegt in der Region Karibik. Dort finden Sie eine Wettervorhersage für die gesamte Region.\n",
      "Sample 14405: Wetter heute, <date> In <city> wird am Morgen die Sonne von einzelnen Wolken verdeckt und die Temperatur liegt bei 5 <temp>. Gegen später gibt es ungestörten Sonnenschein bei Höchstwerten von 25 <temp>. Abends gibt es in <city> überwiegend blauen Himmel mit vereinzelten Wolken bei Temperaturen von 14 bis 20 <temp>. In der Nacht ist es bedeckt bei Tiefsttemperaturen von 12 <temp>. Die Wahrscheinlichkeit für Niederschläge liegt bei 10 <percentile> und es ist mit einer maximalen Niederschlagsmenge von 0.01 <rainfall> zu rechnen. Gefühlt liegen die Temperaturen bei 5 bis 27 <temp>.\n",
      "Sample 28810: Wetter heute, <date> In <city> kann sich am Morgen die Sonne nicht durchsetzen und es bleibt bedeckt bei Werten von 4 <temp>. Später kann sich die Sonne nicht durchsetzen und es bleibt bedeckt und die Temperatur steigt auf 6 <temp>. Abends überwiegt in <city> dichte Bewölkung aber es bleibt trocken bei Werten von 4 bis zu 5 <temp>. Nachts bleibt die Wolkendecke geschlossen bei Tiefstwerten von 4 <temp>. Böen können Geschwindigkeiten zwischen 13 und 19 <velocity> erreichen. Die Wahrscheinlichkeit für Niederschläge liegt bei 90 <percentile> und es ist mit einer maximalen Niederschlagsmenge von 0.07 <rainfall> zu rechnen. Gefühlt liegen die Temperaturen bei 2 bis 8 <temp>. <city> liegt in den Regionen Rhein-Maas-Delta und Nordsee. Öffnen Sie eine Region um eine Wettervorhersage für die gesamte Region zu erhalten.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba83697e1823499f9749886f7558bdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franj\\AppData\\Local\\Temp\\ipykernel_20512\\1245390092.py:210: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Loss: 2.1498\n",
      "\n",
      "Generated examples:\n",
      "Sample 0: Wetter heute, <date> In <city> ist es am bewölkt und die Temperatur liegt bei 22 <temp>. Später scheint die Sonne bei blauem Himmel und die Temperaturen erreichen 3 <temp>. Abends gibt es in <city> Wolken und die Temperaturen liegen zwischen 24 und 27 <temp>. Nachts verdecken einzelne Wolken den Himmel bei Tief\n",
      "Sample 14405: Wetter heute, <date> In <city> überwiegt am Morgen dichte Bewölkung aber es bleibt trocken und die Temperaturen liegen zwischen 3 und 18 <temp>. Nachts ist es wolkig und teils heiter bei Temperaturen von 23 bis zu 14 <temp>. Nachts stören nur nur einzelne Wolken den sonst klaren Himmel bei\n",
      "Sample 28810: Wetter heute, <date> In <city> ist es morgens vielfach wolkig bei Temperaturen von 23 <temp>. Im weiteren Tagesverlauf stören nur einzelne Wolken den sonst blauen Himmel und die Temperaturen erreichen 32 <temp>. <city> liegt in Region der Region Huia. Dort finden Sie eine Wettervorhersage für die gesamte Region. [SEP]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbee09971aa4b62bd3cfa7019f17012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/2:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataLoader with the corrected collate function\n",
    "def create_dataloader(dataset, batch_size, tokenizer):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: prepare_batch(batch, tokenizer)\n",
    "    )\n",
    "\n",
    "class WeatherGRU(nn.Module):\n",
    "    def __init__(self, feature_dim, vocab_size, embedding_dim=256, hidden_dim=512, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.timestep_feature_dim = feature_dim\n",
    "\n",
    "        # Add layer normalization for better stability\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.LayerNorm(self.timestep_feature_dim),\n",
    "            nn.Linear(self.timestep_feature_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Initialize embedding with Xavier/Glorot initialization\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        \n",
    "        # Add gradient clipping to GRU\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Initialize GRU weights\n",
    "        for name, param in self.gru.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        \n",
    "        self.feature_projection = nn.Sequential(\n",
    "            nn.LayerNorm(self.timestep_feature_dim),\n",
    "            nn.Linear(self.timestep_feature_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Add layer normalization before final projection\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, vocab_size)\n",
    "        )\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "    def forward(self, features, tokens, teacher_forcing_ratio=1.0):\n",
    "        # Add input validation\n",
    "        if torch.isnan(features).any():\n",
    "            raise ValueError(\"NaN detected in input features\")\n",
    "        if torch.isinf(features).any():\n",
    "            raise ValueError(\"Inf detected in input features\")\n",
    "            \n",
    "        batch_size = features.size(0)\n",
    "        seq_len = features.size(1)\n",
    "        max_len = tokens.size(1) - 1\n",
    "        \n",
    "        # Scale features to prevent extreme values\n",
    "        features = torch.clamp(features, -10, 10)\n",
    "        \n",
    "        features_reshaped = features.view(-1, self.timestep_feature_dim)\n",
    "        encoded_features = self.feature_encoder(features_reshaped)\n",
    "        encoded_features = encoded_features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Initialize hidden state with scaled features\n",
    "        h_0 = self.feature_projection(features[:, 0])\n",
    "        h_0 = torch.tanh(h_0)  # Ensure values are in [-1, 1]\n",
    "        h_0 = h_0.unsqueeze(0).expand(self.n_layers, batch_size, self.hidden_dim).contiguous()\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, max_len, self.output_layer[-1].out_features, device=features.device)\n",
    "        decoder_input = tokens[:, 0].unsqueeze(1)\n",
    "        \n",
    "        for t in range(max_len):\n",
    "            token_emb = self.embedding(decoder_input)\n",
    "            current_features = encoded_features[:, min(t, seq_len-1)].unsqueeze(1)\n",
    "            \n",
    "            # Scale combined input\n",
    "            combined_input = (token_emb + current_features) / 2\n",
    "            \n",
    "            output, h_0 = self.gru(combined_input, h_0)\n",
    "            \n",
    "            # Check for NaN in hidden state\n",
    "            if torch.isnan(h_0).any():\n",
    "                raise ValueError(f\"NaN detected in hidden state at timestep {t}\")\n",
    "                \n",
    "            prediction = self.output_layer(output)\n",
    "            outputs[:, t:t+1] = prediction\n",
    "            \n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            decoder_input = tokens[:, t+1].unsqueeze(1) if teacher_force else prediction.argmax(dim=-1)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    # Add this method to the WeatherGRU class:\n",
    "    def generate(self, features, max_length=100, temperature=0.7):\n",
    "        \"\"\"\n",
    "        Generate text from features.\n",
    "        \n",
    "        Args:\n",
    "            features: Input features tensor [batch_size, seq_len, feature_dim]\n",
    "            max_length: Maximum length of generated sequence\n",
    "            temperature: Sampling temperature (higher = more random)\n",
    "        \n",
    "        Returns:\n",
    "            list: Generated token indices\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_size = features.size(0)\n",
    "            seq_len = features.size(1)\n",
    "            \n",
    "            # Scale features\n",
    "            features = torch.clamp(features, -10, 10)\n",
    "            \n",
    "            # Process features\n",
    "            features_reshaped = features.view(-1, self.timestep_feature_dim)\n",
    "            encoded_features = self.feature_encoder(features_reshaped)\n",
    "            encoded_features = encoded_features.view(batch_size, seq_len, -1)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            h_0 = self.feature_projection(features[:, 0])\n",
    "            h_0 = torch.tanh(h_0)\n",
    "            h_0 = h_0.unsqueeze(0).expand(self.n_layers, batch_size, self.hidden_dim).contiguous()\n",
    "            \n",
    "            # Ensure decoder_input is a 2D tensor\n",
    "            decoder_input = torch.full((batch_size, 1), tokenizer.cls_token_id, dtype=torch.long, device=features.device)\n",
    "            \n",
    "            generated_tokens = []\n",
    "            \n",
    "            for t in range(max_length):\n",
    "                token_emb = self.embedding(decoder_input)\n",
    "                current_features = encoded_features[:, min(t, seq_len-1)].unsqueeze(1)\n",
    "                combined_input = (token_emb + current_features) / 2\n",
    "                \n",
    "                output, h_0 = self.gru(combined_input, h_0)\n",
    "                logits = self.output_layer(output)\n",
    "                \n",
    "                # Apply temperature\n",
    "                logits = logits.squeeze(1) / temperature\n",
    "                \n",
    "                # Sample from the distribution\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "                \n",
    "                generated_tokens.append(next_token)\n",
    "                \n",
    "                # Stop if we hit the SEP token\n",
    "                if (next_token == tokenizer.sep_token_id).any():\n",
    "                    break\n",
    "                    \n",
    "                decoder_input = next_token\n",
    "            \n",
    "            # Concatenate generated tokens\n",
    "            generated_tokens = torch.cat(generated_tokens, dim=1)\n",
    "            return generated_tokens\n",
    "\n",
    "def prepare_batch(batch_list, tokenizer):\n",
    "    features = torch.stack([item['features'] for item in batch_list])\n",
    "    texts = [item['text'] for item in batch_list]\n",
    "    \n",
    "    # Normalize features\n",
    "    features = (features - features.mean()) / (features.std() + 1e-8)\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'features': features,\n",
    "        'text': encoded['input_ids']\n",
    "    }\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, teacher_forcing_ratio=1.0, epoch=0, total_epochs=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        try:\n",
    "            features = batch['features'].to(device)\n",
    "            text = batch['text'].to(device)\n",
    "            \n",
    "            # Check for invalid values in inputs\n",
    "            if torch.isnan(features).any() or torch.isinf(features).any():\n",
    "                print(f\"Warning: Invalid values in features at batch {batch_idx}\")\n",
    "                continue\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with gradient checking\n",
    "            with torch.autograd.detect_anomaly():\n",
    "                outputs = model(features, text, teacher_forcing_ratio)\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                targets = text[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Check if loss is valid\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"Warning: Invalid loss value {loss.item()} at batch {batch_idx}\")\n",
    "                    print(\"Last output values:\", outputs[-5:])\n",
    "                    print(\"Last target values:\", targets[-5:])\n",
    "                    raise ValueError(\"Invalid loss detected\")\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Check gradients\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        grad_norm = param.grad.norm()\n",
    "                        if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n",
    "                            print(f\"Warning: Invalid gradient for {name}\")\n",
    "                            raise ValueError(f\"Invalid gradient detected in {name}\")\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                avg_loss = total_loss / num_batches\n",
    "                pbar.set_postfix({\"Loss\": f\"{avg_loss:.4f}\"})\n",
    "                \n",
    "        except ValueError as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "\n",
    "# Training setup with adjusted hyperparameters\n",
    "def train_model(model, dataset, tokenizer, num_epochs=10, batch_size=16, learning_rate=1e-4):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Use a smaller batch size and learning rate for stability\n",
    "    dataloader = create_dataloader(clean_dataset, batch_size=batch_size, tokenizer=tokenizer)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='mean')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Add learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    # Select a few examples for generation testing\n",
    "    test_indices = [0, len(dataset)//2, len(dataset)-1]  # Beginning, middle, and end\n",
    "    test_samples = [dataset[i] for i in test_indices]\n",
    "    test_features = torch.stack([sample['features'] for sample in test_samples]).to(device)\n",
    "    \n",
    "    print(\"\\nOriginal texts for test samples:\")\n",
    "    for idx, sample in zip(test_indices, test_samples):\n",
    "        print(f\"Sample {idx}: {sample['text']}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        try:\n",
    "            loss = train_epoch(\n",
    "                model, dataloader, criterion, optimizer, device,\n",
    "                teacher_forcing_ratio=0.9,\n",
    "                epoch=epoch, total_epochs=num_epochs\n",
    "            )\n",
    "            losses.append(loss)\n",
    "            \n",
    "            scheduler.step(loss)\n",
    "            print(f\"\\nEpoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            # Generate examples\n",
    "            print(\"\\nGenerated examples:\")\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = model.generate(test_features, temperature=0.7)\n",
    "                for idx, tokens in enumerate(generated_tokens):\n",
    "                    generated_text = tokenizer.decode(tokens, skip_special_tokens=False)\n",
    "                    print(f\"Sample {test_indices[idx]}: {generated_text}\")\n",
    "            model.train()\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                }, f'checkpoint_epoch_{epoch+1}.pt')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during epoch {epoch + 1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Usage example:\n",
    "model = WeatherGRU(\n",
    "    feature_dim=22,\n",
    "    vocab_size=len(tokenizer),\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "    n_layers=2\n",
    ")\n",
    "\n",
    "losses = train_model(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    num_epochs=2,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Iteration\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "\n",
    "ca 9,806,800 parameter laut Claude\n",
    "\n",
    "Results:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
