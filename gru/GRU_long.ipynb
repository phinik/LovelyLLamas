{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from GRU_collection import BasicWeatherGRU as WeatherGRU\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "# Special tokens for text substitution\n",
    "special_tokens = {\n",
    "    'additional_special_tokens': ['<city>','<temp>','<date>','<velocity>','<percentile>','<rainfall>', '<ne>']\n",
    "}\n",
    "\n",
    "# Add special tokens into tokenizer\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, weather_data, max_length=100):\n",
    "        self.data = [weather_data] if isinstance(weather_data, dict) else weather_data\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize wind directions from all data points\n",
    "        self.wind_directions = sorted(list(set([d for data in self.data for d in data['windrichtung']])))\n",
    "        self.wind_dir_to_idx = {d: i for i, d in enumerate(self.wind_directions)}\n",
    "        \n",
    "        # Calculate expected feature dimension\n",
    "        self.feature_dim = (\n",
    "            1 +  # temperature\n",
    "            1 +  # rain risk\n",
    "            1 +  # rain amount\n",
    "            1 +  # wind speed\n",
    "            1 +  # pressure\n",
    "            1 +  # humidity\n",
    "            1 +  # cloudiness\n",
    "            len(self.wind_directions) +  # one-hot wind directions\n",
    "            2 +  # time encoding (sin, cos)\n",
    "            3 +  # sun features\n",
    "            1    # sun hours\n",
    "        )\n",
    "        \n",
    "        # Initialize spaCy NER model for German text\n",
    "        try:\n",
    "            import spacy\n",
    "            print(\"Loading German NER model...\")\n",
    "            self.nlp = spacy.load(\"de_core_news_lg\")\n",
    "            self.has_ner = True\n",
    "            print(\"NER model loaded successfully\")\n",
    "        except:\n",
    "            print(\"Warning: Could not load spaCy model. Make sure to install it with:\")\n",
    "            print(\"pip install spacy\")\n",
    "            print(\"python -m spacy download de_core_news_lg\")\n",
    "            self.has_ner = False\n",
    "    \n",
    "    def replace_dates(self, text: str) -> str:\n",
    "        text = re.sub(r\"\\b\\d{1,2}\\.\\d{1,2}\\.\\d{4}\\b\", \"<date>\", text)\n",
    "        return text\n",
    "    \n",
    "    def apply_ner_replacement(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Apply Named Entity Recognition to replace geographic entities with <ne> tag\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            str: Text with geographic entities replaced by <ne> tag\n",
    "        \"\"\"\n",
    "        if not self.has_ner:\n",
    "            return text\n",
    "            \n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Collect entities and their positions\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"LOC\", \"GPE\"]:  # Locations and Geopolitical entities\n",
    "                entities.append((ent.start_char, ent.end_char, ent.text))\n",
    "        \n",
    "        # Sort entities by start position in reverse order to avoid offset issues\n",
    "        entities.sort(reverse=True, key=lambda x: x[0])\n",
    "        \n",
    "        # Replace each entity with <ne> tag\n",
    "        for start, end, entity in entities:\n",
    "            text = text[:start] + \"<ne>\" + text[end:]\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def replace_city_and_units(self, text: str, city: str) -> str:\n",
    "        # Replace the city name with <city> tag\n",
    "        # Make sure city isn't empty before replacing\n",
    "        if city and len(city) > 0:\n",
    "            text = re.sub(r'\\b' + re.escape(city) + r'\\b', '<city>', text)\n",
    "\n",
    "        unit_patterns = [\n",
    "        # TEMPERATURE\n",
    "        (r'(°[ ]*C|Grad)', ' <temp>'),\n",
    "        # VELOCITY\n",
    "        (r'[ ]*km/h', ' <velocity>'),\n",
    "        # PERCENTILE\n",
    "        (r'[ ]*%', ' <percentile>'),\n",
    "        # RAINFALL DELTA\n",
    "        # (r'\\d+\\.\\d+ bis \\d+\\.\\d+[ ]*l\\/m²', '<rainfall> bis <rainfall>'),\n",
    "        (r'[ ]*l\\/m²', ' <rainfall>')\n",
    "        ]\n",
    "        for pattern, replacement in unit_patterns:\n",
    "            try:\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        # REMOVE MARKUP\n",
    "        text = re.sub(r'\\**', '', text)\n",
    "\n",
    "        # REMOVE WEIRD PUNCUATION\n",
    "        text = re.sub(r' \\.', '.', text)\n",
    "\n",
    "        # REMOVE UNNECESSARY NEWLINES\n",
    "        text = re.sub(r'\\n\\n', '\\n', text)\n",
    "\n",
    "        # REMOVE SPACE AFTER NEWLINE\n",
    "        text = re.sub(r'\\n ', '\\n', text)\n",
    "\n",
    "        # REPLACE MULTIPLE WHITESPACES WITH ONE\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def _parse_time(self, time_str):\n",
    "        \"\"\"Parse time string and handle missing data\"\"\"\n",
    "        if time_str == '-' or not time_str:\n",
    "            return None\n",
    "        try:\n",
    "            # Handle \"HH:MM Uhr\" format\n",
    "            if ':' in time_str:\n",
    "                hour, minute = map(int, time_str.split(' ')[0].split(':'))\n",
    "                return hour + minute/60\n",
    "            return None\n",
    "        except (ValueError, IndexError):\n",
    "            return None\n",
    "\n",
    "    def _encode_time(self, time_str):\n",
    "        # Convert \"HH - HH Uhr\" to cyclic features\n",
    "        try:\n",
    "            start_hour = int(time_str.split(' - ')[0])\n",
    "            hour_sin = torch.sin(torch.tensor(2 * math.pi * start_hour / 24))\n",
    "            hour_cos = torch.cos(torch.tensor(2 * math.pi * start_hour / 24))\n",
    "            return torch.tensor([hour_sin, hour_cos])\n",
    "        except (ValueError, IndexError):\n",
    "            # Return neutral values for invalid time\n",
    "            return torch.tensor([0.0, 1.0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _encode_sun_info(self, sunrise, sunset, current_time):\n",
    "        # Parse times, handling missing data\n",
    "        sunrise_hour = self._parse_time(sunrise)\n",
    "        sunset_hour = self._parse_time(sunset)\n",
    "        \n",
    "        try:\n",
    "            current_hour = float(current_time.split(' - ')[0])\n",
    "        except (ValueError, IndexError):\n",
    "            # Return default values if current time is invalid\n",
    "            return torch.tensor([0.0, 0.0, 0.0])\n",
    "        \n",
    "        # If sunrise or sunset is missing, use approximate values based on season\n",
    "        if sunrise_hour is None or sunset_hour is None:\n",
    "            # Return default encoding indicating uncertainty\n",
    "            return torch.tensor([\n",
    "                0.5,  # Unknown daylight status\n",
    "                0.0,  # Neutral time since sunrise\n",
    "                0.0   # Neutral time until sunset\n",
    "            ])\n",
    "        \n",
    "        # Calculate daylight features\n",
    "        is_daylight = (current_hour >= sunrise_hour) and (current_hour <= sunset_hour)\n",
    "        \n",
    "        if is_daylight:\n",
    "            time_since_sunrise = (current_hour - sunrise_hour) / (sunset_hour - sunrise_hour)\n",
    "            time_until_sunset = (sunset_hour - current_hour) / (sunset_hour - sunrise_hour)\n",
    "        else:\n",
    "            if current_hour < sunrise_hour:\n",
    "                time_since_sunrise = -1 * (sunrise_hour - current_hour) / (24 - sunset_hour + sunrise_hour)\n",
    "                time_until_sunset = -1\n",
    "            else:\n",
    "                time_since_sunrise = -1\n",
    "                time_until_sunset = -1 * (current_hour - sunset_hour) / (24 - sunset_hour + sunrise_hour)\n",
    "        \n",
    "        return torch.tensor([float(is_daylight), time_since_sunrise, time_until_sunset])\n",
    "\n",
    "    def one_hot_wind(self, wind_dir):\n",
    "        encoding = torch.zeros(len(self.wind_directions))\n",
    "        encoding[self.wind_dir_to_idx[wind_dir]] = 1\n",
    "        return encoding\n",
    "    \n",
    "    def process_text(self, text, city):\n",
    "        \"\"\"Process text with all replacements in the correct order\"\"\"\n",
    "        # First replace city and units \n",
    "        processed_text = self.replace_city_and_units(text, city)\n",
    "        \n",
    "        # Then replace dates\n",
    "        processed_text = self.replace_dates(processed_text)\n",
    "        \n",
    "        # CRITICAL: Handle NER replacement properly\n",
    "        # We need to ensure that the model can properly recognize <ne> tags\n",
    "        # First, check if we have NER capability\n",
    "        if self.has_ner:\n",
    "            # Get all current special token positions\n",
    "            special_tokens = ['<city>', '<temp>', '<date>', '<velocity>', '<percentile>', '<rainfall>']\n",
    "            protected_regions = []\n",
    "            \n",
    "            for token in special_tokens:\n",
    "                start_idx = 0\n",
    "                while start_idx < len(processed_text):\n",
    "                    pos = processed_text.find(token, start_idx)\n",
    "                    if pos == -1:\n",
    "                        break\n",
    "                    protected_regions.append((pos, pos + len(token)))\n",
    "                    start_idx = pos + len(token)\n",
    "            \n",
    "            # Sort protected regions\n",
    "            protected_regions.sort()\n",
    "            \n",
    "            # Create a list of unprotected text regions that can be processed with NER\n",
    "            unprotected_regions = []\n",
    "            last_end = 0\n",
    "            \n",
    "            for start, end in protected_regions:\n",
    "                if start > last_end:\n",
    "                    unprotected_regions.append((last_end, start))\n",
    "                last_end = end\n",
    "            \n",
    "            # Add the final region\n",
    "            if last_end < len(processed_text):\n",
    "                unprotected_regions.append((last_end, len(processed_text)))\n",
    "            \n",
    "            # Process each unprotected region with NER\n",
    "            result_text = \"\"\n",
    "            last_pos = 0\n",
    "            \n",
    "            for start, end in unprotected_regions:\n",
    "                # Add any special tokens before this region\n",
    "                result_text += processed_text[last_pos:start]\n",
    "                \n",
    "                # Process this region with NER\n",
    "                region_text = processed_text[start:end]\n",
    "                doc = self.nlp(region_text)\n",
    "                \n",
    "                # Collect entities in this region\n",
    "                entities = []\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_ in [\"LOC\", \"GPE\"]:  # Locations and Geopolitical entities\n",
    "                        entities.append((ent.start_char, ent.end_char, ent.text))\n",
    "                \n",
    "                # Sort entities by start position in reverse order\n",
    "                entities.sort(reverse=True, key=lambda x: x[0])\n",
    "                \n",
    "                # Replace entities with <ne> tag\n",
    "                region_result = region_text\n",
    "                for ent_start, ent_end, _ in entities:\n",
    "                    region_result = region_result[:ent_start] + \"<ne>\" + region_result[ent_end:]\n",
    "                \n",
    "                result_text += region_result\n",
    "                last_pos = end\n",
    "            \n",
    "            # Add any remaining text\n",
    "            result_text += processed_text[last_pos:]\n",
    "            processed_text = result_text\n",
    "        \n",
    "        return processed_text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Get sequence length from the data\n",
    "        seq_len = len(item['temperatur_in_deg_C'])\n",
    "        \n",
    "        # Initialize features tensor with correct shape\n",
    "        features = torch.zeros((seq_len, self.feature_dim))\n",
    "        \n",
    "        # Fill features one by one, maintaining consistent shapes\n",
    "        current_idx = 0\n",
    "        \n",
    "        # Numerical features - all should be shape [seq_len, 1]\n",
    "        features[:, current_idx] = torch.tensor([float(t) for t in item['temperatur_in_deg_C']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        features[:, current_idx] = torch.tensor([float(r) for r in item['niederschlagsrisiko_in_perc']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        # Handle rain amount with forward filling for NaN values\n",
    "        rain_values = []\n",
    "        last_valid = 0.0\n",
    "        for r in item['niederschlagsmenge_in_l_per_sqm']:\n",
    "            try:\n",
    "                val = float(r)\n",
    "                if not torch.isnan(torch.tensor(val)):\n",
    "                    last_valid = val\n",
    "                rain_values.append(last_valid)\n",
    "            except ValueError:\n",
    "                rain_values.append(last_valid)\n",
    "        features[:, current_idx] = torch.tensor(rain_values)\n",
    "        current_idx += 1\n",
    "        \n",
    "        features[:, current_idx] = torch.tensor([float(w) for w in item['windgeschwindigkeit_in_km_per_s']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        features[:, current_idx] = torch.tensor([float(p) for p in item['luftdruck_in_hpa']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        features[:, current_idx] = torch.tensor([float(h) for h in item['relative_feuchte_in_perc']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        features[:, current_idx] = torch.tensor([float(c.split('/')[0]) / 8 for c in item['bewölkungsgrad']])\n",
    "        current_idx += 1\n",
    "        \n",
    "        # Wind directions (one-hot encoded)\n",
    "        wind_features = torch.stack([self.one_hot_wind(w) for w in item['windrichtung']])\n",
    "        features[:, current_idx:current_idx + len(self.wind_directions)] = wind_features\n",
    "        current_idx += len(self.wind_directions)\n",
    "        \n",
    "        # Time features\n",
    "        time_features = torch.stack([self._encode_time(t) for t in item['times']])\n",
    "        features[:, current_idx:current_idx + 2] = time_features\n",
    "        current_idx += 2\n",
    "        \n",
    "        # Sun features\n",
    "        sun_features = torch.stack([\n",
    "            self._encode_sun_info(\n",
    "                item.get('sunrise', '-'), \n",
    "                item.get('sundown', '-'), \n",
    "                t\n",
    "            ) for t in item['times']\n",
    "        ])\n",
    "        features[:, current_idx:current_idx + 3] = sun_features\n",
    "        current_idx += 3\n",
    "        \n",
    "        # Sun hours feature\n",
    "        sun_hours = torch.tensor([1.0 if \"fast nicht zu sehen\" in item.get('sunhours', '') else 0.0])\n",
    "        features[:, current_idx] = sun_hours.expand(seq_len)\n",
    "        \n",
    "        # Process the text using the NER-based method\n",
    "        processed_text = self.process_text(item['report_long'], item['city'])\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'text': processed_text\n",
    "        }\n",
    "    \n",
    "    def scan_dataset_for_named_entities(self, sample_size=None):\n",
    "        \"\"\"\n",
    "        Scan the dataset to identify common named entities\n",
    "        \n",
    "        Args:\n",
    "            sample_size: Number of samples to analyze, None for all\n",
    "            \n",
    "        Returns:\n",
    "            Counter of named entities and their frequencies\n",
    "        \"\"\"\n",
    "        if not self.has_ner:\n",
    "            print(\"NER model not available. Cannot scan for named entities.\")\n",
    "            return None\n",
    "            \n",
    "        from collections import Counter\n",
    "        \n",
    "        # Get samples to analyze\n",
    "        if sample_size is None:\n",
    "            samples = self.data\n",
    "        else:\n",
    "            import random\n",
    "            samples = random.sample(self.data, min(sample_size, len(self.data)))\n",
    "            \n",
    "        # Collect all entities\n",
    "        all_entities = []\n",
    "        \n",
    "        for i, item in enumerate(samples):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processing sample {i}/{len(samples)}...\")\n",
    "                \n",
    "            doc = self.nlp(item['report_long'])\n",
    "            \n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in [\"LOC\", \"GPE\"]:\n",
    "                    all_entities.append(ent.text)\n",
    "        \n",
    "        # Return counter of entities\n",
    "        return Counter(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8172c45e4f5e4d1bb46c71d1b6474738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    # change directory if not on root\n",
    "    if str(os.getcwd()).endswith('LLamas') == False:\n",
    "        os.chdir('../..')\n",
    "    \n",
    "    # check if dict contains correct key\n",
    "    # def check_file(path):\n",
    "    #     with open(path, 'r', encoding='utf-8') as f:\n",
    "    #         data: dict = json.load(f)\n",
    "    #         if 'gpt_rewritten_v2' in data.keys():\n",
    "    #             return True\n",
    "    #         else:\n",
    "    #             return False\n",
    "\n",
    "    def check_file(path):\n",
    "        \"\"\"\n",
    "        Checks if the file contains valid keys and values.\n",
    "        Returns True if the file should be loaded, False otherwise.\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data: dict = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Invalid JSON in file: {path}\")\n",
    "                return False\n",
    "\n",
    "            # Ensure required keys are present\n",
    "            if not {'report_long', 'city'}.issubset(data.keys()):\n",
    "                return False\n",
    "\n",
    "            # Check if 'city' and 'report_long' have valid non-empty values\n",
    "            if not isinstance(data['city'], str) or not data['city'].strip():\n",
    "                return False\n",
    "            if not isinstance(data['report_long'], str) or not data['report_long'].strip():\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "                \n",
    "    # transform data into weatherDataSet class format\n",
    "    def load_data(path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data: dict = json.load(f)\n",
    "            return data\n",
    "    \n",
    "    # files for reading\n",
    "    files = os.listdir(os.path.join(os.getcwd(), 'data', 'files_for_chatGPT', '2024-12-12'))\n",
    "    files = {(file.split('-')[-1]).split('_')[0]:load_data(os.path.join(os.getcwd(), 'data', 'files_for_chatGPT', '2024-12-12', file)) for file in tqdm(files) if check_file(os.path.join(os.getcwd(), 'data', 'files_for_chatGPT', '2024-12-12', file))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading German NER model...\n",
      "NER model loaded successfully\n",
      "Processing sample 0/1000...\n",
      "Processing sample 100/1000...\n",
      "Processing sample 200/1000...\n",
      "Processing sample 300/1000...\n",
      "Processing sample 400/1000...\n",
      "Processing sample 500/1000...\n",
      "Processing sample 600/1000...\n",
      "Processing sample 700/1000...\n",
      "Processing sample 800/1000...\n",
      "Processing sample 900/1000...\n",
      "Counter({'Karibik': 153, 'Werten': 52, 'Temperaturen': 52, 'Nachts': 18, 'Kapverden': 7, 'Santa Fe': 6, 'California': 6, 'Catalina': 5, 'Trinidad': 5, 'Las Mercedes': 5, 'San Pedro': 4, 'La Rinconada': 4, 'Morgen Wolken': 4, 'San Pablo': 4, 'Italienische Adria': 4, 'San Agustín': 4, 'San Antonio': 4, 'Florida': 4, 'San Lorenzo': 4, 'Pueblo Nuevo': 4, 'Tierra Blanca': 4, 'La Esperanza': 4, 'Stetten SH': 3, 'Santa Bárbara': 3, '°C': 3, 'Montão': 3, 'Malveira': 3, 'Grande Tête Rouge': 3, 'Los Caballos': 3, 'Au Parc': 3, 'Baliares': 3, 'Preveza': 3, 'Golden Grove': 3, 'El Salado': 3, 'Mata de Felipito': 3, 'Saint Lucia': 3, 'La Source': 3, 'Vistabella': 3, 'Mata de Palma': 3, 'Egg SZ': 3, 'Maulti': 3, 'Grove Town': 3, 'Union': 3, 'Puerto Escondido': 3, 'Rancho al Medio': 3, 'Los Ranchitos': 3, 'La Magdalena': 3, 'Hardbargain': 3, 'Duncan': 3, 'La Venta': 3, 'Eugendorf': 3, 'Blecke': 3, 'La Brusca': 3, 'Mount Agnes': 3, 'Colonia': 3, 'Sainte Croix': 3, 'El Corte': 3, 'Castle Hill': 3, 'Turquino': 3, 'La Tourney': 3, 'Bertin': 3, 'Ortona': 3, 'Edey': 3, 'Mount Thompson': 3, 'Villarpando': 3, 'Savannes Estate': 3, 'Toril': 3, 'Fray Benito': 3, 'Micoud': 3, 'Geagans': 3, 'Pelican Point': 3, 'Lead Vale': 3, 'La Llanada de Puerto Santo': 3, 'Rum': 3, 'Port-au-Prince': 3, 'Angelina': 3, 'Crawford': 3, 'Zaanstad': 3, 'Nordsee': 3, 'Oistins': 3, 'La Lechuza': 3, 'Dinsley': 3, 'Cliftons': 3, 'Cayucal de La Antonia': 3, 'Saint David': 3, 'San Luis': 3, 'Hayes': 3, 'Maturita': 3, 'Matayaya': 3, 'Vuelta Grande': 3, 'Weston': 3, 'El Caño': 3, 'Urlings': 3, 'Carlton': 3, 'Pinar del Río': 3, 'El Chorro': 3, 'Fahmine': 3, 'Mearnsville': 3, 'Mülhausen': 3, 'Dee Side': 3, 'Reykir': 3, 'Los Cambrones Derrumbado': 3, 'Bishops': 3, 'Morne Aux Frégates': 3, 'Danache': 3, 'Carrera Limón': 3, 'Antigua Saratoga': 3, 'Belle Plain': 3, 'Cheltenham': 3, 'El Colorado': 3, 'Canaan': 3, 'Grangwav': 3, 'Los Chicharrones': 3, 'Biljana': 3, 'Guáimaro': 3, 'Mahot': 3, 'La Cayota': 3, 'Woodbourne': 3, 'Pattersons': 3, 'Morpo': 3, 'Santo António': 3, 'Foremost': 3, 'La Fragata': 3, 'Malgrétoute': 3, 'Degand': 3, 'Los Dos Brazos': 3, 'South District': 3, 'Figueira': 3, 'Naus': 3, 'Sinagoga': 3, 'Palero': 3, 'Vela Luka': 3, 'Cumberbatch': 3, 'Fellowship Hall': 3, 'Lacorne': 3, 'La Salud': 3, 'Mão para Trás': 3, 'Nan la Peine': 3, 'Jicomé Arriba': 3, 'Platta': 3, 'Hellingen': 3, 'La Fleur': 3, 'Newark': 3, 'Duquesne': 3, 'Vöcklamarkt': 3, 'Ilirska Bistrica': 3, 'La Palma Arriba': 3, 'Għajn Żejtuna': 3, 'Cara Linda': 3, 'Desruisseaux': 3, 'Cotuí': 3, 'Santa Capuza': 3, 'Cabbage Point': 3, 'Despinoze': 3, 'Cayon': 3, 'Altagracia': 3, 'La Romera': 3, 'Genovés': 3, 'Amancio': 3, 'Baiguate': 3, 'Bain Town': 3, 'La Baría': 3, 'Cancino': 3, 'Baflo': 3, 'Asentista': 3, 'Bevans Town': 3, 'Los Peralejos': 3, 'Diélé': 2, 'Hochrhein': 2, 'Bodensee': 2, 'Schwarzwald': 2, 'Warre Creek': 2, 'Puerto Limón': 2, 'Watri': 2, 'El Centro': 2, 'Lethbridge': 2, 'Kiskunmajsa': 2, 'Rockstone': 2, 'Santa Elena': 2, 'Alagoa Grande': 2, 'Starigrad': 2, 'Morón': 2, 'Kemmerer': 2, 'Santa Rita': 2, 'New Maryland': 2, 'Downpatrick': 2, 'Mopile': 2, 'Dole pri Litiji': 2, 'Agua Fría': 2, 'Lima': 2, 'Capar': 2, 'Tanghin-Dassouri': 2, 'Vierwaldstättersee': 2, 'Luzerner See': 2, 'Anrode': 2, 'El Progreso': 2, 'Brezovica': 2, 'Sundbyberg': 2, 'Ostumán': 2, 'Mamolj': 2, 'Koforidua': 2, 'Chico Cimarrón': 2, 'Lèsoumaso': 2, 'Nuevo San Antonio': 2, 'Vridi': 2, 'Guayaquil': 2, 'Las Palmas': 2, 'Krmalnik': 2, 'Crobas': 2, 'Bolama': 2, 'Cumayasa Kilómetros Diez': 2, 'Čokrlije': 2, 'Agogo': 2, 'Tarqui': 2, 'Sabinas': 2, 'Estancia Río Negro': 2, 'Stamford': 2, 'Brickfield': 2, 'Piripiri': 2, 'Asis': 2, 'Jocoro': 2, 'Bowen': 2, 'Chaguaramas': 2, 'Merrill': 2, 'San Luis de Palqui': 2, 'Lake Charles': 2, 'Chagüitillo': 2, 'Acul du Nord': 2, 'Portugalete': 2, 'Casilda': 2, 'La Esmeralda': 2, 'Mieszkowice': 2, 'Wineperu': 2, 'Jaz': 2, 'Ocotales': 2, 'Viana do Castelo': 2, 'Villa Concepción': 2, 'Aberdeen': 2, 'Moussala': 2, 'La Quinta': 2, 'Pachón': 2, 'Logradouro': 2, 'Gozo': 2, 'Malta': 2, 'Sibun Camp': 2, 'San Francisco de los Romo': 2, 'Angol': 2, 'Springfield': 2, 'Pozo del Indio': 2, 'Salitral': 2, 'Petrópolis': 2, 'Iowa City': 2, 'Potomac': 2, 'Las Jarretaderas': 2, 'Springhill': 2, 'Hopital': 2, 'Aïn Harrouda': 2, 'Puerto Adela': 2, 'Istrien': 2, 'Ocoyo': 2, 'Dupont': 2, 'Sra': 2, 'Ventanas': 2, 'Comasagua': 2, 'Detroit': 2, 'Santa Cruz del Quiché': 2, 'Mingali': 2, 'Canton': 2, 'Gundinci': 2, 'Bessanah': 2, 'Douar Iachouraïne': 2, 'Jipijapa': 2, 'Walensee': 2, 'Heidiland': 2, 'Keur Babou': 2, 'Quebrada Seca': 2, 'Recreo': 2, 'Rioja': 2, 'Skutskär': 2, 'Saint Johns': 2, 'Ngomedzap': 2, 'La Bolsa': 2, 'San Francisco': 2, 'Anastasia': 2, 'Grecco': 2, 'Cody': 2, 'Elizaveta': 2, 'Playa Grande': 2, 'Pakira-Kondre': 2, 'Kallinge': 2, 'Pahiatua': 2, 'Baso': 2, 'Greeley': 2, 'Riohacha': 2, 'Chacarita': 2, 'Haouadid': 2, 'Salir de Matos': 2, 'Cerro de Oro': 2, 'Santo Domingo': 2, 'Nebraska City': 2, 'Buenos Aires': 2, 'Aït Fdila': 2, 'Fuschlsee': 2, 'Mondsee': 2, 'Salzkammergut': 2, 'Guajayare': 2, 'Wildenschwert': 2, 'Cuatro Vientos': 2, 'Farakala': 2, 'Bandéa': 2, 'Novo Pôrto': 2, 'San Pedro de la Costa': 2, 'Puebla': 2, 'Fairfax Village': 2, 'Tobolo': 2, 'Ricanau Petro Ondro': 2, 'Río Marabasco': 2, 'Forquilhinha': 2, 'Fornos de Algodres': 2, 'Téguérana': 2, 'Matapalo': 2, 'San José de Cusmapa': 2, 'Juliaca': 2, 'Bainbridge': 2, 'La Cuesta': 2, 'Cuba': 2, 'Ballisodare': 2, 'Alsózsolca': 2, 'Ciudad Piar': 2, 'Iray': 2, 'Orangeburg': 2, 'South Carolina': 2, 'Valle Nuevo': 2, 'Draga': 2, 'Goundi': 2, 'Soloma': 2, 'Parinari': 2, 'Menditahun': 2, 'Bahía Murta': 2, 'Hacienda Santa Rosa': 2, 'La Saca': 2, 'Chitagá': 2, 'Kibungo': 2, 'Colca': 2, 'Canindé de São Francisco': 2, 'Skenderovići': 2, 'Villa Mainero': 2, 'Kurri': 2, 'Berkovci': 2, 'Barranquillita': 2, 'San Bernardo': 2, 'Ventura': 2, 'Brezno': 2, 'Villa de Costa Rica': 2, 'Kapori': 2, 'Asafiri Kondre': 2, 'Meridian': 2, 'Paranavaí': 2, 'Besali': 2, 'Straža': 2, 'Rio Bananal': 2, 'Rüegsau': 2, 'Los Telares': 2, 'Llano Largo': 2, 'Kuísaruhu': 2, 'Aveleda': 2, 'Boula Kakadi': 2, 'Cruz': 2, 'Castelo de Vide': 2, 'Valle Jocotillo': 2, 'Atima': 2, 'Cornelio': 2, 'Vainilla': 2, 'Pymble': 2, 'Colonia Joya de Cerén': 2, 'Quimichapa': 2, 'Bangor': 2, 'Washington': 2, 'Hacha': 2, 'Treguaco': 2, 'Flandreau': 2, 'Oteru': 2, 'Safana': 2, 'La Trinitaria': 2, 'Peshkopi': 2, 'Casas': 2, 'Kandion Mangana': 2, 'Pásiga': 2, 'New Orleans': 2, 'Tato': 2, 'Tankanto': 2, 'Ollagüe': 2, 'Los Tamariscos': 2, 'Videira': 2, 'Sakassou': 2, 'Waimea': 2, 'Gongko': 2, 'Puente Alto': 2, 'Melo': 2, 'Vapa': 2, 'San Luis de Sincé': 2, 'Dayton': 2, 'Puerto Baquerizo Moreno': 2, 'Bari': 2, 'Babira': 2, 'Zrâfiyé': 2, 'Espírito Santo': 2, 'Agua Blanca': 2, 'Tabrichat': 2, 'Blainville': 2, 'Cicaíno': 2, 'Kuzmica': 2, 'Güinope': 2, 'La Colmena': 2, 'Babonneau': 2, 'Animal Flower Cave': 2, 'Szentes': 2, 'Nurdup': 2, 'Zaw': 2, 'Taounza': 2, 'Guamo': 2, 'Codpa': 2, 'Estancia Pindoty': 2, 'El Rosario': 2, 'Mixquiahuala de Juarez': 2, 'Tolimán': 2, 'Maporal': 2, 'Giasabli': 2, 'Okegawa': 2, 'Hofors': 2, 'Tadó': 2, 'Ote': 2, 'Tres de Mayo': 2, 'Camatambo': 2, 'Maribondo': 2, 'Hede': 2, 'Rotnes': 2, 'Comalapa': 2, 'Liboje': 2, 'Poblado Lavalleja': 2, 'Berikovo': 2, 'Pipillipai': 2, 'La Unión': 2, 'Apron-Pronou': 2, 'Roguemnogo': 2, 'Rivadavia': 2, 'Cuilapan de Guerrero': 2, 'Sidi Yahya Ou Saad': 2, 'Chalmeca': 2, 'Manassas Park': 2, 'San José de Villa': 2, 'Zoura': 2, 'Santa Rosa de Toay': 2, 'Santa Ana Arriba': 2, 'Tallaght': 2, 'Saint Charles': 2, 'Itabac': 2, 'Wittikamba': 2, 'São Félix do Xingu': 2, 'Vaughan': 2, 'Menominee': 2, 'Rabón': 2, 'El Dovio': 2, 'San Ignacio de Tupile': 2, 'Senica': 2, 'Valdivia': 2, 'Cariblanco': 2, 'Hacienda La Calera': 2, 'La Fincona': 2, 'Putaendo': 2, 'Savannah': 2, 'Georgia': 2, 'La Paloma': 2, 'Salapa': 2, 'Wamu': 2, 'Reading': 2, 'Los Peguero': 2, 'Henndi': 2, 'Pewo': 2, 'Gornja Dobrilovina': 2, 'Ramadilla': 2, 'La Boca de Laguna Grande': 2, 'Valle San Juan de Somoto': 2, 'Mankono': 2, 'Maitencillo': 2, 'Nombre de Jesús': 2, 'Santiago Nonualco': 2, 'Richland Park': 2, 'Jucanyá': 2, 'New Home': 2, 'Semelhe': 2, 'Zoui': 2, 'El Pezote': 2, 'Leeds': 2, 'Ronchi': 2, 'Drevenik': 2, 'Aklavik': 2, 'Neves': 2, 'Zeltweg': 2, 'Palo Alto': 2, 'Marmato': 2, 'Cuilapa': 2, 'Puerto de Oro': 2, 'Troya': 2, 'Mount Vernon': 2, 'Sincelejo': 2, 'La Democracia': 2, 'Pempe': 2, 'Wassu': 2, 'Palmar Sur': 2, 'Bamako': 2, 'Samborondón': 2, 'Hinnerup': 2, 'Cosrou': 2, 'Los Ángeles': 2, 'Abram-Village': 2, 'Pietrasanta': 2, 'Maragua': 2, 'Puerto Wilches': 2, 'Old Orchard Beach': 2, 'Estancia Sol Naciente': 2, 'Samec': 2, 'El Pueblo Viejo': 2, 'Tisovec': 2, 'Linton': 2, 'Kanzra': 2, 'San Rafael': 2, 'Santa Lucía': 2, 'Ibirité': 2, 'Iztapalapa': 2, 'Cocodrilo': 2, 'Huité': 2, 'Chehalis': 2, 'Nadadores': 2, 'Ogfia': 2, 'Sarandí del Yi': 2, 'Piedras Negras': 2, 'Hailey': 2, 'Yavorodji': 2, 'Santa Barbara': 2, 'Xeparquiy': 2, 'San Gabriel': 2, 'Salvatierra': 2, 'Župa': 2, 'Puerto Castilla': 2, 'El Mihan': 2, 'Ceres': 2, 'Ingienkondre': 2, 'Minden': 2, 'San Miguelito': 2, 'Ojo de Agua': 2, 'Burgos': 2, 'Regidor': 2, 'La Cruz': 2, 'Ayaviri': 2, 'El Palmar de Cariaco': 2, 'Peralta': 2, 'Baria': 2, 'Ipu': 2, 'Fellabaer': 2, 'La Joya': 2, 'Hlebce': 2, 'Brezni Vrh': 2, 'Sidi Allal Tazi': 2, 'Puerto San Julián': 2, 'Bitono': 2, 'El Picacho': 2, 'La Albardilla': 2, 'Caño Hondo': 2, 'Jonjoso': 2, 'El Bolsón': 2, 'Bethesda': 2, 'Offinso': 2, 'Pantoja': 2, 'Tabou': 2, 'Capão Verde': 2, 'Tagima': 2, 'Libertad': 2, 'Calvaire': 2, 'Itapetinga': 2, 'Melville': 2, 'Obia': 2, 'Kiseljak': 2, 'Potopoto': 2, 'Las Cañadas': 2, 'Uchuguaico': 2, 'Tulita': 2, 'Urcuquí': 2, 'Retalhuleu': 2, 'Siuna': 2, 'Isiokpo': 2, 'Puerto San Antonio': 2, 'Harouna': 2, 'Westerly': 2, 'Jaraguá do Sul': 2, 'Celbridge': 2, 'Oré Fello': 2, 'Carson City': 2, 'Abtao': 2, 'Wéta': 2, 'Nobres': 2, 'La Albina': 2, 'Irecê': 2, 'Copalillo': 2, 'Manzanillo': 2, 'Buenavista': 2, 'Jaro': 2, 'Trojas': 2, 'Guaitarilla': 2, 'Clonakilty': 2, 'Gravataí': 2, 'Garki': 2, 'Aguini': 2, 'Masparrito': 2, 'Cerro Banco': 2, 'Mansion': 2, 'Waterbury': 2, 'Villa Ojo de Agua': 2, 'La Bandera': 2, 'Ellendale': 2, 'Cerros de Vera': 2, 'Santa Maria Nativitas': 2, 'Kearney': 2, 'Saran': 2, 'São Pedro de Alva': 2, 'Ceriale': 2, 'Olintepeque': 2, 'Gravel Bay': 2, 'Nuevo Ideal': 2, 'Jaboncillo': 2, 'Hacienda Samanga de Sevilla': 2, 'Derry Village': 2, 'Tenerife': 2, 'Cotacachi': 2, 'Dona Inês': 2, 'Ciudad General Escobedo': 2, 'Cumbernauld': 2, 'Paucarcolla': 2, 'Stonewall': 2, 'Yawada': 2, 'Tonosí': 2, 'San Cristóbal': 2, 'Karlovac': 2, 'Las Margaritas': 2, 'San Felipe': 2, 'Tereré': 2, 'Pulqui': 2, 'La Curbina': 2, 'Baquedano': 2, 'Malečnik': 2, 'Mount Pleasant': 2, 'El Cerrito': 2, 'Venustiano Carranza': 2, 'Marlborough': 2, 'Ebi': 2, 'Edinburgh': 2, 'Llicllao': 2, 'Cachoeira': 2, 'Punta Novillo': 2, 'Gavião': 2, 'Kongola': 2, 'Ouaninou': 2, 'Chenwrinken': 2, 'Mezgalji': 2, 'Madras': 2, 'Urisirima': 2, 'Aparecida do Taboado': 2, 'Rama': 2, 'Lircay Arriba': 2, 'Guaduas': 2, 'Diamantina': 2, 'Gasselternijveen': 2, 'Huitán': 2, 'San Vicente': 2, 'Kurutuku Village': 2, 'Kroatische Adria': 2, 'Bellavista': 2, 'Kokerite': 2, 'Villa Darwin': 2, \"N'Djari\": 2, 'Caraballeda': 2, 'Santani': 2, 'Spodnji Duplek': 2, 'Saxacalli': 2, 'Bois-Guillaume': 2, 'Guaviyú': 2, 'Chongoyape': 2, 'La Virginia': 2, 'Pirešica': 2, 'Bath': 2, 'Fongbé Apédomé': 2, 'Surumatra': 2, 'Rayón': 2, 'El Cobre': 2, 'Las Nubes': 2, 'Orotaka': 2, 'Abari': 2, 'Fužina': 2, 'Nkinkaso': 2, 'Bote': 2, 'Churchill': 2, 'Zlatoličje': 2, 'Cabuya': 2, 'Nová Paka': 2, 'Ejido Mayocoba': 2, 'Kimbo': 2, 'Campo Grande': 2, 'Stendal': 2, 'Hillsboro': 2, 'Calzada': 2, 'La Mocha': 2, 'Tranebjerg': 2, 'Villa La Angostura': 2, 'Las Flores': 2, 'Exchange': 2, 'Montañita': 2, 'San Sebastián Abajo': 2, 'True Blue': 2, 'Canyar': 2, 'Santa Cruz do Sul': 2, 'Pajara': 2, 'Samacá': 2, 'Puerto Pirámides': 2, 'El Ayote': 2, 'Brifu': 2, 'Zuenda': 2, 'Gornje Zaostro': 2, 'Dahioké': 2, 'Río Frío': 2, 'America': 2, 'Scavino': 2, 'Svätý': 2, 'Hildesheim': 2, 'Crowley': 2, 'La Azucena': 2, 'Cárdenas': 2, 'Punta Peña': 2, 'San Pedro Columbia': 2, 'York': 2, 'Soulémaka': 2, 'Issikro': 2, 'Smithfield': 2, 'Sillapata': 2, 'Connersville': 2, 'Norra Bro': 2, 'Stara Cesta': 2, 'El Toro': 2, 'Cancuncal': 2, 'Ulapa': 2, 'Isla Wichitupo Grande': 2, 'Reumén': 2, 'Bethany': 2, 'Quispamsis': 2, 'Nda Akissikro': 2, 'San Diego': 2, 'El Rincón': 2, 'San Rafael de Onoto': 2, 'Los Lazos': 2, 'Santa Rosa de Oro': 2, 'Taxisco': 2, 'Makawao': 2, 'Mbang': 2, 'La Espigadilla': 2, 'Arévalo': 2, 'El Aouina': 2, 'Kayah': 2, 'Lour': 2, 'Quinas': 2, 'Taminango': 2, 'Gumel': 2, 'Salem': 2, 'Puesto Valle': 2, 'La Vela de Coro': 2, 'Nevesinje': 2, 'Isla de Maipo': 2, 'La Anguila': 2, 'Boraure': 2, 'Cosamaloapan': 2, 'Sebastopol': 2, 'Four Winds': 2, 'Arrën': 2, 'Tetecala': 2, 'Villamaría': 2, 'Huejutla de Reyes': 2, 'Gävle': 2, 'Rivera': 2, 'Farim': 2, 'Vitória': 2, 'Andaray': 2, 'Dyersburg': 2, 'Šulan': 2, 'Palermo': 2, 'Jummu Town': 2, 'Fafa': 2, 'Bella Cruz': 2, 'Fort Benton': 2, 'Tres Cruces': 2, 'Desaguadero': 2, 'Heywoods': 2, 'Ciudad Choluteca': 2, 'Billiot': 2, 'Vila dos Operários': 2, 'Funza': 2, 'Syèngou': 2, 'Francisco Vela': 2, 'Kassiri': 2, 'Los Chiles': 2, 'Touba Ndiaye': 2, 'Gadalo': 2, 'Safim': 2, 'Rinconada de Los Andes': 2, 'İnegöl': 2, 'Tagua': 2, 'Podgozd': 2, 'Llacllin': 2, 'Vrhole': 2, 'Konjicah': 2, 'Los Varela': 2, 'Roanoke': 2, 'Abomoso': 2, 'Wrigley': 2, 'Alvarães': 2, 'Effurun': 2, 'Santiago': 2, 'Hafnarfjörður': 2, 'Chiapas': 2, 'Slottsbron': 2, 'Turčianske Teplice': 2, 'Guéagui': 2, 'San Diego de la Unión': 2, 'Stratford': 2, 'Afraré': 2, 'Garin Bassam': 2, 'Soledad de Graciano Sánchez': 2, 'Villa Media Agua': 2, 'Gradac': 2, 'Möttling': 2, 'Morococha': 2, 'Arrifana': 2, 'Roma': 2, 'La Colorada': 2, 'Embarcadero': 2, 'La Cañada': 2, 'Saramiriza': 2, 'Sabanilla': 2, 'Tioukongueul': 2, 'Benton': 2, 'Guanay': 2, 'Nuevo Vallarta': 2, 'Bishopville': 2, 'Aïn el Khadra': 2, 'Santa Rosalía': 2, 'Noblesville': 2, 'Wilmington': 2, 'Delaware': 2, 'Valle de Trujillo': 2, 'Villa Industrial': 2, 'Temanto Spri': 2, 'Girón': 2, 'Ivanjševski Vrh': 2, 'Edgartown': 2, 'Ribadesella': 2, 'Pathawaru': 2, 'Schaqui': 2, 'Ganboul Moctar': 2, 'Drapers': 2, 'La Cruz de Las Peñitas': 2, 'El Malcal': 2, 'Campo de La Texas': 2, 'Poblado del Turupí': 2, 'Puerto Botánico': 2, 'Boca del Río': 2, 'Rosso': 2, 'Pryor': 1, 'Hegau': 1, 'Bodanrück': 1, 'Tawanta': 1, 'Tawanta bei Werten': 1, 'Nanaimo': 1, 'Talant': 1, 'Talant Wolkenfelder': 1, 'Santa Bárbara der Himmel': 1, 'Du': 1, 'Fort Resolution': 1, 'San Isidro': 1, 'San Isidro Regen': 1, 'El Porvenir': 1, 'Stock': 1, 'Belford Roxo': 1, 'Ionische Inseln': 1, 'Oumm': 1, 'Oumm Kchéra': 1, 'Cumayasa': 1, 'Kilómetros Diez': 1, 'Manfi': 1, 'Manfi der Himmel': 1, 'Bowen Sound': 1, 'Acul du Nord bewölkt': 1, 'Casilda Teile des Himmels': 1, 'Querévalo': 1, 'Cobija': 1, 'Tomaj': 1, 'Tecate': 1, 'Posic': 1, 'Sayaxché': 1, 'Zürichsee': 1, 'Xique Xique': 1, 'Sara Kunda': 1, 'Sara Kunda Wolkenfelder': 1, 'Hof': 1, 'Tanekore Village': 1, 'Tanekore': 1, 'La Azulita': 1, 'Cañadas': 1, 'Cañadas der Himmel': 1, 'Berchtesgadener Land': 1, 'Wolfgangsee': 1, 'Parco': 1, 'Wichub Uala': 1, 'Wichub': 1, 'Godim': 1, 'Godim der Himmel': 1, 'Čoveča Glava': 1, 'Čoveča Glava Nebel': 1, 'Camps': 1, 'Asoampa': 1, 'Portalegre': 1, 'Portalegre Wolkenfelder': 1, 'San José': 1, 'San José Teile des Himmels': 1, 'Campo Alegre': 1, 'Tarumá': 1, 'Tarumá Regen': 1, 'Carmen Inés': 1, 'Carmelo': 1, 'Colonia Emilio Williams': 1, 'Sahela Foukania': 1, 'Oupeye': 1, 'El Paraíso': 1, 'Albania': 1, 'Babonneau bewölkt': 1, 'Animal Flower Cave Wolkenfelder': 1, 'Munkfors': 1, 'Munkfors der Himmel': 1, 'Sobral': 1, 'Stubaital': 1, 'Achensee': 1, 'Zugspitze': 1, 'Zillertal': 1, 'Oberbayern': 1, 'of War Sound': 1, 'Los Pozotes': 1, 'San José de Villa wolkenlos': 1, 'IJsselmeer': 1, 'Rhein-Maas-Delta': 1, 'Potrero Guazú': 1, 'Garglor': 1, 'Fercal': 1, 'Los Peguero wolkenlos': 1, 'Teggaz': 1, 'Cavaliers': 1, 'Huariaca': 1, 'Richland': 1, 'Umuahia': 1, 'Ronchi dei Legionari': 1, 'Colonia Salto': 1, 'Colonia Salto der Schirm': 1, 'Vrh pri': 1, 'Santa Luzia': 1, 'Santa Luzia Regen': 1, 'Cocodrilo Teile des Himmels': 1, 'Oševek': 1, 'Oševek Nebel': 1, 'Djerba': 1, 'Breisgau': 1, 'Elsass': 1, 'Oberrhein': 1, 'Guanaco Pampa': 1, 'Pullingue': 1, 'Pullingue wolkenlos': 1, 'Militar': 1, 'Pelundo': 1, 'Santa Lucia La Reforma': 1, 'Santa Lucia La Reforma der Himmel': 1, 'Arctic Coast Way': 1, 'Yahualica de González Gallo': 1, 'Pôrto Seguro': 1, 'San Miguel Tepezontes': 1, 'San Miguel': 1, 'Hlebce Wolkenfelder': 1, 'Wörthersee': 1, 'Colonia R. Scagnoli': 1, 'Caño Hondo Regen': 1, 'Yokoboué': 1, 'Hall': 1, 'Calvaire wolkenlos': 1, 'Colonia Juancho': 1, 'Télébokan': 1, 'Abdón Castro Tolay': 1, 'Comandante Portillo': 1, 'Comandante Portillo Regen': 1, 'La Troncal': 1, 'La Troncal Regen': 1, 'Komou': 1, 'Komou Teile des Himmels': 1, 'Guarapari': 1, 'Guarapari Regen': 1, 'Bete': 1, 'Mount Pleasant Teile des Himmels': 1, 'Suva': 1, 'Mbocayá': 1, 'Vice': 1, 'Almaguer': 1, 'Pirayú': 1, 'Hunsrück': 1, 'Cruzeiro do Sul': 1, 'Cruzeiro do Sul Wolkenfelder': 1, 'Playón Chico': 1, 'Playón': 1, 'Santa Catalina': 1, 'Santa Catalina Regen': 1, 'Fuerteventura': 1, 'Kanaren': 1, 'Béli-Tiovi': 1, 'Ensanche La Esperanza': 1, 'Barfa': 1, 'Attersee': 1, 'Kvarner Bucht': 1, 'Pya': 1, 'Pya wolkenlos': 1, 'Skalité': 1, 'La Anguila der Himmel': 1, 'Sambang': 1, 'Sambang Wolkenfelder': 1, 'Heywoods Wolkenfelder': 1, 'Billiot der Himmel': 1, 'Jarpuken': 1, 'Keur Babakar Toumbou': 1, 'Keur Babakar Toumbou Wolkenfelder': 1, 'Major Isidoro': 1, 'Major Isidoro Wolkenfelder': 1, 'Tabaconas': 1, 'Podmolnik': 1, 'Abba': 1, 'Jackson': 1, 'Mamitupu': 1, 'Mamitupu der Schirm': 1, 'Borkum': 1, 'Wattenmeer': 1, 'Touldé Boussobé': 1, 'Cuchilla': 1, 'Cuchilla Manguera': 1, 'Pampa del Indio': 1, 'Drapers Regen': 1, 'El Paso': 1})\n",
      "Loading German NER model...\n",
      "NER model loaded successfully\n",
      "\n",
      "Validation Summary:\n",
      "Total samples: 28811\n",
      "Samples with issues: 43\n",
      "Total NaN values: 454\n",
      "Total infinite values: 0\n",
      "Total extreme values: 0\n",
      "\n",
      "Removed 43 samples\n",
      "Remaining samples: 28768\n",
      "Loading German NER model...\n",
      "NER model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "weather_data = list(files.values())\n",
    "dataset = WeatherDataset(weather_data, max_length=100)\n",
    "# Optionally analyze entities in the dataset\n",
    "entity_counts = dataset.scan_dataset_for_named_entities(sample_size=1000)\n",
    "print(entity_counts)\n",
    "def validate_and_clean_weather_data(weather_data, dataset_class):\n",
    "    \"\"\"\n",
    "    Validates the dataset and returns cleaned weather_data with problematic samples removed.\n",
    "    \n",
    "    Args:\n",
    "        weather_data: List of weather data samples\n",
    "        dataset_class: The dataset class constructor to use for validation\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (cleaned_weather_data, removed_indices, validation_summary)\n",
    "    \"\"\"\n",
    "    # Create temporary dataset for validation\n",
    "    temp_dataset = dataset_class(weather_data, max_length=100)\n",
    "    \n",
    "    summary = {\n",
    "        'total_samples': len(temp_dataset),\n",
    "        'invalid_samples': [],\n",
    "        'statistics': {\n",
    "            'nan_count': 0,\n",
    "            'inf_count': 0,\n",
    "            'extreme_values': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    invalid_indices = set()\n",
    "    \n",
    "    # Validate each sample\n",
    "    for idx in range(len(temp_dataset)):\n",
    "        try:\n",
    "            sample = temp_dataset[idx]\n",
    "            features = sample['features']\n",
    "            \n",
    "            has_issue = False\n",
    "            \n",
    "            # Check for NaN values\n",
    "            nan_mask = torch.isnan(features)\n",
    "            if nan_mask.any():\n",
    "                summary['statistics']['nan_count'] += nan_mask.sum().item()\n",
    "                has_issue = True\n",
    "                \n",
    "            # Check for infinity\n",
    "            inf_mask = torch.isinf(features)\n",
    "            if inf_mask.any():\n",
    "                summary['statistics']['inf_count'] += inf_mask.sum().item()\n",
    "                has_issue = True\n",
    "                \n",
    "            # Check for extreme values\n",
    "            extreme_mask = (features.abs() > 1e6)\n",
    "            if extreme_mask.any():\n",
    "                summary['statistics']['extreme_values'] += extreme_mask.sum().item()\n",
    "                has_issue = True\n",
    "            \n",
    "            if has_issue:\n",
    "                invalid_indices.add(idx)\n",
    "                summary['invalid_samples'].append({\n",
    "                    'index': idx,\n",
    "                    'text': sample['text']\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "            invalid_indices.add(idx)\n",
    "            summary['invalid_samples'].append({\n",
    "                'index': idx,\n",
    "                'error_message': str(e)\n",
    "            })\n",
    "    \n",
    "    # Create cleaned weather_data list\n",
    "    cleaned_weather_data = [\n",
    "        data for idx, data in enumerate(weather_data) \n",
    "        if idx not in invalid_indices\n",
    "    ]\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nValidation Summary:\")\n",
    "    print(f\"Total samples: {summary['total_samples']}\")\n",
    "    print(f\"Samples with issues: {len(summary['invalid_samples'])}\")\n",
    "    print(f\"Total NaN values: {summary['statistics']['nan_count']}\")\n",
    "    print(f\"Total infinite values: {summary['statistics']['inf_count']}\")\n",
    "    print(f\"Total extreme values: {summary['statistics']['extreme_values']}\")\n",
    "    print(f\"\\nRemoved {len(invalid_indices)} samples\")\n",
    "    print(f\"Remaining samples: {len(cleaned_weather_data)}\")\n",
    "    \n",
    "    return cleaned_weather_data, list(invalid_indices), summary\n",
    "\n",
    "# Example usage function\n",
    "def clean_and_create_dataset(weather_data, dataset_class):\n",
    "    \"\"\"\n",
    "    Cleans the weather data and creates a new dataset.\n",
    "    \n",
    "    Args:\n",
    "        weather_data: Original weather data list\n",
    "        dataset_class: Dataset class to use\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (new_dataset, removed_indices, validation_summary)\n",
    "    \"\"\"\n",
    "    cleaned_data, removed_indices, summary = validate_and_clean_weather_data(\n",
    "        weather_data, dataset_class\n",
    "    )\n",
    "    \n",
    "    # Create new dataset with cleaned data\n",
    "    clean_dataset = dataset_class(cleaned_data, max_length=100)\n",
    "    \n",
    "    return clean_dataset, removed_indices, summary\n",
    "\n",
    "# Clean the data and create new dataset\n",
    "clean_dataset, removed_indices, summary = clean_and_create_dataset(weather_data, WeatherDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_vocabulary(tokenizer, dataset, batch_size=64):\n",
    "    \"\"\"Identify used tokens and create a reduced vocabulary mapping\"\"\"\n",
    "    print(\"Analyzing vocabulary usage to reduce model size...\")\n",
    "    \n",
    "    from collections import Counter\n",
    "    from tqdm.notebook import tqdm\n",
    "    import torch\n",
    "    \n",
    "    # Count tokens\n",
    "    token_counter = Counter()\n",
    "    \n",
    "    # Process the entire dataset\n",
    "    for idx in tqdm(range(len(dataset)), desc=\"Scanning token usage\"):\n",
    "        # Get the raw text directly\n",
    "        if isinstance(dataset, torch.utils.data.Subset):\n",
    "            sample = dataset.dataset[dataset.indices[idx]]\n",
    "        else:\n",
    "            sample = dataset[idx]\n",
    "        \n",
    "        text = sample['text']\n",
    "        \n",
    "        # Tokenize directly\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        token_counter.update(tokens)\n",
    "    \n",
    "    # Always keep special tokens\n",
    "    for special_token in tokenizer.special_tokens_map.values():\n",
    "        if isinstance(special_token, str):\n",
    "            token_id = tokenizer.convert_tokens_to_ids(special_token)\n",
    "            if token_id not in token_counter:\n",
    "                token_counter[token_id] = 1\n",
    "        elif isinstance(special_token, list):\n",
    "            for token in special_token:\n",
    "                token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "                if token_id not in token_counter:\n",
    "                    token_counter[token_id] = 1\n",
    "    \n",
    "    # Sort by frequency for efficient token ID assignment\n",
    "    used_token_ids = sorted(token_counter.keys())\n",
    "    \n",
    "    # Create token ID mapping (old ID -> new ID)\n",
    "    token_id_map = {old_id: new_id for new_id, old_id in enumerate(used_token_ids)}\n",
    "    \n",
    "    # Create reverse mapping for inference\n",
    "    reverse_token_id_map = {new_id: old_id for old_id, new_id in token_id_map.items()}\n",
    "    \n",
    "    # Store the mappings for later use\n",
    "    token_mappings = {\n",
    "        'token_id_map': token_id_map,\n",
    "        'reverse_token_id_map': reverse_token_id_map,\n",
    "        'used_token_ids': used_token_ids\n",
    "    }\n",
    "    \n",
    "    # Update vocabulary size to reduced size\n",
    "    reduced_vocab_size = len(used_token_ids)\n",
    "    original_vocab_size = len(tokenizer.vocab)\n",
    "    print(f\"Reduced vocabulary from {original_vocab_size:,} to {reduced_vocab_size:,} tokens \" \n",
    "          f\"({reduced_vocab_size/original_vocab_size*100:.1f}%)\")\n",
    "    \n",
    "    return token_mappings, reduced_vocab_size\n",
    "\n",
    "# Map tokens function for data loaders\n",
    "def map_tokens_fn(batch, token_id_map):\n",
    "    \"\"\"Map token IDs to new reduced IDs\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    # Map the token IDs to new IDs\n",
    "    old_tokens = batch['text']\n",
    "    new_tokens = torch.zeros_like(old_tokens)\n",
    "    \n",
    "    # Apply mapping\n",
    "    for i in range(old_tokens.size(0)):\n",
    "        for j in range(old_tokens.size(1)):\n",
    "            old_id = old_tokens[i, j].item()\n",
    "            new_tokens[i, j] = token_id_map.get(old_id, 0)  # Default to 0 if token not found\n",
    "    \n",
    "    batch['text'] = new_tokens\n",
    "    return batch\n",
    "\n",
    "# Modified prepare_batch function to use token mapping\n",
    "def prepare_batch_with_mapping(batch_list, tokenizer, token_id_map=None):\n",
    "    \"\"\"Prepare batch with optional token mapping for reduced vocabulary\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    features = torch.stack([item['features'] for item in batch_list])\n",
    "    texts = [item['text'] for item in batch_list]\n",
    "    \n",
    "    # Normalize features\n",
    "    features = (features - features.mean()) / (features.std() + 1e-8)\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    batch = {\n",
    "        'features': features,\n",
    "        'text': encoded['input_ids']\n",
    "    }\n",
    "    \n",
    "    # Apply token mapping if provided\n",
    "    if token_id_map is not None:\n",
    "        batch = map_tokens_fn(batch, token_id_map)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Create DataLoader with token mapping support\n",
    "def create_dataloader_with_mapping(dataset, batch_size, tokenizer, token_id_map=None):\n",
    "    \"\"\"Create a DataLoader with optional token mapping\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: prepare_batch_with_mapping(batch, tokenizer, token_id_map)\n",
    "    )\n",
    "\n",
    "def count_model_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in the model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def adjust_model_size(feature_dim, vocab_size, embedding_dim, hidden_dim, max_params=30_000_000):\n",
    "    \"\"\"Adjust model dimensions to stay under parameter limit\"\"\"\n",
    "    import math\n",
    "    \n",
    "    # Calculate WeatherGRU model parameter count\n",
    "    # Main parameters:\n",
    "    # - Feature encoder: feature_dim * embedding_dim * 2 (two linear layers)\n",
    "    # - Embedding: vocab_size * embedding_dim\n",
    "    # - GRU: embedding_dim * hidden_dim * 3 (input projection) + hidden_dim * hidden_dim * 3 (recurrent)\n",
    "    # - Output layer: hidden_dim * vocab_size\n",
    "    \n",
    "    param_count = (\n",
    "        feature_dim * embedding_dim * 2 +  # Feature encoder (two layers)\n",
    "        vocab_size * embedding_dim +       # Embedding layer\n",
    "        (embedding_dim * hidden_dim * 3) + # GRU input projection (3 gates)\n",
    "        (hidden_dim * hidden_dim * 3) +    # GRU recurrent connections (3 gates)\n",
    "        hidden_dim * vocab_size            # Output layer\n",
    "    )\n",
    "    \n",
    "    print(f\"Initial parameter count: {param_count:,}\")\n",
    "    \n",
    "    if param_count <= max_params:\n",
    "        return embedding_dim, hidden_dim\n",
    "    \n",
    "    # If we need to reduce params, try to find optimal dimensions\n",
    "    # Strategy: reduce both embedding_dim and hidden_dim proportionally\n",
    "    \n",
    "    # Calculate reduction factor needed\n",
    "    reduction_factor = math.sqrt(max_params / param_count)\n",
    "    \n",
    "    # Apply reduction\n",
    "    new_embedding_dim = max(32, int(embedding_dim * reduction_factor))\n",
    "    new_hidden_dim = max(64, int(hidden_dim * reduction_factor))\n",
    "    \n",
    "    # Recalculate to verify\n",
    "    new_param_count = (\n",
    "        feature_dim * new_embedding_dim * 2 +  \n",
    "        vocab_size * new_embedding_dim +       \n",
    "        (new_embedding_dim * new_hidden_dim * 3) + \n",
    "        (new_hidden_dim * new_hidden_dim * 3) +    \n",
    "        new_hidden_dim * vocab_size            \n",
    "    )\n",
    "    \n",
    "    print(f\"Adjusted dimensions: embedding_dim={new_embedding_dim}, hidden_dim={new_hidden_dim}\")\n",
    "    print(f\"Adjusted parameter count: {new_param_count:,}\")\n",
    "    \n",
    "    return new_embedding_dim, new_hidden_dim\n",
    "\n",
    "def save_model_with_metadata(model, path, token_mappings=None, config=None):\n",
    "    \"\"\"Save model with all required metadata for later use\"\"\"\n",
    "    import torch\n",
    "    import os\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Prepare save data\n",
    "    save_data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # Add token mappings if available\n",
    "    if token_mappings is not None:\n",
    "        save_data['token_mappings'] = token_mappings\n",
    "    \n",
    "    # Add model configuration if available\n",
    "    if config is not None:\n",
    "        save_data['config'] = config\n",
    "    \n",
    "    # Save to file\n",
    "    torch.save(save_data, path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "    \n",
    "    return path\n",
    "\n",
    "def load_model_with_metadata(path, device='cpu'):\n",
    "    \"\"\"Load model with all metadata\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    # Load saved data\n",
    "    save_data = torch.load(path, map_location=device)\n",
    "    \n",
    "    return save_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOMALY DETECTION RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Analyzing vocabulary usage to reduce model size...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e74a6e3af0842f2997aad916dc3faee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning token usage:   0%|          | 0/28768 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced vocabulary from 30,007 to 444 tokens (1.5%)\n",
      "Initial parameter count: 1,531,904\n",
      "Model has 1,609,236 trainable parameters\n",
      "\n",
      "Original texts for test samples:\n",
      "Sample 0: Wetter heute, <date> In <city> stören am Morgen nur einzelne Wolken den sonst blauen Himmel bei Temperaturen von 23 <temp>. Mittags stören nur einzelne Wolken den sonst blauen Himmel und die Temperaturen erreichen 30 <temp>. Abends gibt es in <city> lockere Bewölkung bei Temperaturen von 24 bis 26 <temp>. In der Nacht bedecken einzelne Wolken den Himmel und die Werte gehen auf 22 <temp> zurück. Die gefühlten Temperaturen liegen bei 23 bis 33 <temp>. <city> liegt in der Region <ne>. Dort finden Sie eine Wettervorhersage für die gesamte Region.\n",
      "Sample 14384: Wetter heute, <date> In <city> ist es morgens sonnig und die Temperatur liegt bei 21 <temp>. Im Laufe des Mittags scheint die Sonne bei blauem Himmel und das Thermometer klettert auf 27 <temp>. Abends gibt es in <city> einen wolkenlosen Himmel bei Temperaturen von 22 bis 24 <temp>. Nachts ist es klar und die Werte gehen auf 20 <temp> zurück. Die Niederschlagswahrscheinlichkeit liegt bei 15 <percentile>, während mit einer Niederschlagsmenge von maximal 0.11 <rainfall> zu rechnen ist. Gefühlt liegen die Temperaturen bei 20 bis 29 <temp>. <city> liegt in der Region <ne>. Dort finden Sie eine Wettervorhersage für die gesamte Region.\n",
      "Sample 28767: Wetter heute, <date> In <city> kann sich am Morgen die Sonne nicht durchsetzen und es bleibt bedeckt bei Werten von 4 <temp>. Später kann sich die Sonne nicht durchsetzen und es bleibt bedeckt und die Temperatur steigt auf 6 <temp>. Abends überwiegt in <city> dichte Bewölkung aber es bleibt trocken bei Werten von 4 bis zu 5 <temp>. Nachts bleibt die Wolkendecke geschlossen bei Tiefstwerten von 4 <temp>. Böen können Geschwindigkeiten zwischen 13 und 19 <velocity> erreichen. Die Wahrscheinlichkeit für Niederschläge liegt bei 90 <percentile> und es ist mit einer maximalen Niederschlagsmenge von 0.07 <rainfall> zu rechnen. Gefühlt liegen die Temperaturen bei 2 bis 8 <temp>. <city> liegt in den Regionen <ne> und <ne>. Öffnen Sie eine Region um eine Wettervorhersage für die gesamte Region zu erhalten.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f180afd365774217b20f8f7429bdb454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franj\\AppData\\Local\\Temp\\ipykernel_27164\\967178375.py:205: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Loss: 1.1792\n",
      "\n",
      "Generated examples:\n",
      "Sample 0: Wetter heute, <date> In <city> regnet es Höchst erreicht 28 <temp>. Am Abend regnet es in <city> bei Werten von 26 bis zu 27 <temp>. Nachts gibt es Regen bei Tiefsttemperaturen von 22 <temp>. Mit einer Wahrscheinlichkeit von 15 <percentile>, ist über den Tag verteilt mit Niederschlagsmengen von 0. 01 bis 0. 07 <rainfall> zu rechnen. Gefühlt liegen die Temperaturen bei 26 bis 29 <temp>. <city> liegt in der Region <ne>. Dort finden Sie eine Wettervorhersage\n",
      "Sample 14384: Wetter heute, <date> In <city> regnet es Höchst erreicht 37 <temp>. Am Abend gibt es in <city> Regen bei Werten von 21 bis zu 23 <temp>. Nachts gibt es Regen bei Tiefstwerten von 22 <temp>. Mit einer Wahrscheinlichkeit von 15 <percentile>, ist über den Tag verteilt mit Niederschlagsmengen von 0. 01 bis 0. 29 <rainfall> zu rechnen. Gefühlt liegen die Temperaturen bei 23 bis 30 <temp>. <city> liegt in der Region <ne>. Dort finden Sie eine Wettervorhersage für\n",
      "Sample 28767: Wetter heute, <date> In <city> bleibt morgens die Wolkendecke geschlossen bei <ne> von - 1 <temp>. Nachts ist es neblig trüb bei Tiefstwerten von - 1 <temp>. Böen können Geschwindigkeiten zwischen 6 und 9 <velocity> erreichen. Gefühlt liegen die Temperaturen bei - 2 bis - 1 <temp>. [SEP] liegt in den Regionen <ne> und <ne>. Öffnen Sie eine Region um eine Wettervorhersage für die gesamte Region zu erhalten. [SEP] liegt in den Regionen <ne> und <ne>. Öffnen\n",
      "Model saved to models\\best_gru_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader with token mapping support\n",
    "def create_dataloader_with_mapping(dataset, batch_size, tokenizer, token_id_map=None):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: prepare_batch_with_mapping(batch, tokenizer, token_id_map)\n",
    "    )\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, teacher_forcing_ratio=1.0, epoch=0, total_epochs=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        try:\n",
    "            features = batch['features'].to(device)\n",
    "            text = batch['text'].to(device)\n",
    "            \n",
    "            # Check for invalid values in inputs\n",
    "            if torch.isnan(features).any() or torch.isinf(features).any():\n",
    "                print(f\"Warning: Invalid values in features at batch {batch_idx}\")\n",
    "                continue\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with gradient checking\n",
    "            with torch.autograd.detect_anomaly():\n",
    "                outputs = model(features, text, teacher_forcing_ratio)\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                targets = text[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Check if loss is valid\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"Warning: Invalid loss value {loss.item()} at batch {batch_idx}\")\n",
    "                    print(\"Last output values:\", outputs[-5:])\n",
    "                    print(\"Last target values:\", targets[-5:])\n",
    "                    raise ValueError(\"Invalid loss detected\")\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Check gradients\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        grad_norm = param.grad.norm()\n",
    "                        if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n",
    "                            print(f\"Warning: Invalid gradient for {name}\")\n",
    "                            raise ValueError(f\"Invalid gradient detected in {name}\")\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                avg_loss = total_loss / num_batches\n",
    "                pbar.set_postfix({\"Loss\": f\"{avg_loss:.4f}\"})\n",
    "                \n",
    "        except ValueError as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "\n",
    "# Updated train_model function with vocabulary reduction and parameter management\n",
    "def train_model(dataset, tokenizer, max_params=3_000_000, num_epochs=10, batch_size=64, learning_rate=1e-3):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # First, reduce vocabulary\n",
    "    token_mappings, reduced_vocab_size = reduce_vocabulary(tokenizer, dataset, batch_size)\n",
    "    \n",
    "    # Get feature dimension\n",
    "    feature_dim = dataset.feature_dim\n",
    "    \n",
    "    # Determine optimal model dimensions based on parameter constraints\n",
    "    embedding_dim, hidden_dim = adjust_model_size(\n",
    "        feature_dim=feature_dim,\n",
    "        vocab_size=reduced_vocab_size,\n",
    "        embedding_dim=256,  # Starting point\n",
    "        hidden_dim=512,    # Starting point\n",
    "        max_params=max_params\n",
    "    )\n",
    "    \n",
    "    # Create model with adjusted dimensions\n",
    "    model = WeatherGRU(\n",
    "        feature_dim=feature_dim,\n",
    "        vocab_size=reduced_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_layers=1,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    param_count = count_model_parameters(model)\n",
    "    print(f\"Model has {param_count:,} trainable parameters\")\n",
    "    \n",
    "    # Create dataloader with token mapping\n",
    "    dataloader = create_dataloader_with_mapping(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        tokenizer=tokenizer,\n",
    "        token_id_map=token_mappings['token_id_map']\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='mean')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Add learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Select a few examples for generation testing\n",
    "    test_indices = [0, len(dataset)//2, len(dataset)-1]  # Beginning, middle, and end\n",
    "    test_samples = [dataset[i] for i in test_indices]\n",
    "    test_features = torch.stack([sample['features'] for sample in test_samples]).to(device)\n",
    "    \n",
    "    print(\"\\nOriginal texts for test samples:\")\n",
    "    for idx, sample in zip(test_indices, test_samples):\n",
    "        print(f\"Sample {idx}: {sample['text']}\")\n",
    "    \n",
    "    # Create directory for model checkpoints\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        try:\n",
    "            loss = train_epoch(\n",
    "                model, dataloader, criterion, optimizer, device,\n",
    "                teacher_forcing_ratio=0.9,\n",
    "                epoch=epoch, total_epochs=num_epochs\n",
    "            )\n",
    "            losses.append(loss)\n",
    "            \n",
    "            scheduler.step(loss)\n",
    "            print(f\"\\nEpoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            # Generate examples\n",
    "            print(\"\\nGenerated examples:\")\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = model.generate(\n",
    "                    test_features,\n",
    "                    token_mappings=token_mappings\n",
    "                )\n",
    "                for idx, tokens in enumerate(generated_tokens):\n",
    "                    # Map tokens back to original vocabulary\n",
    "                    original_tokens = [token_mappings['reverse_token_id_map'][t.item()] for t in tokens]\n",
    "                    generated_text = tokenizer.decode(original_tokens, skip_special_tokens=False)\n",
    "                    print(f\"Sample {test_indices[idx]}: {generated_text}\")\n",
    "            model.train()\n",
    "            \n",
    "            # Save checkpoint if it's the best model so far\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                config = {\n",
    "                    'feature_dim': feature_dim,\n",
    "                    'vocab_size': reduced_vocab_size,\n",
    "                    'embedding_dim': embedding_dim,\n",
    "                    'hidden_dim': hidden_dim,\n",
    "                    'n_layers': 1,\n",
    "                    'dropout': 0.2,\n",
    "                    'epoch': epoch,\n",
    "                    'loss': loss\n",
    "                }\n",
    "                \n",
    "                save_model_with_metadata(\n",
    "                    model=model,\n",
    "                    path=os.path.join('models', 'best_gru_model.pt'),\n",
    "                    token_mappings=token_mappings,\n",
    "                    config=config\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during epoch {epoch + 1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return model, token_mappings, losses\n",
    "\n",
    "# Usage example:\n",
    "# Note that we now call train_model differently, passing dataset and tokenizer directly\n",
    "model, token_mappings, losses = train_model(\n",
    "    dataset=clean_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_params=30_000_000,  # 3 million parameter limit\n",
    "    num_epochs=1, # tends to overfit after 1 epoch\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Iteration\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "\n",
    "ca 9,806,800 parameter laut Claude\n",
    "\n",
    "Results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franj\\AppData\\Local\\Temp\\ipykernel_27164\\1388653163.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  save_data = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 24299:\n",
      "Original: Wetter heute, <date> In <city> kann sich morgens die Sonne nicht durchsetzen und es bleibt bedeckt bei Werten von -1 <temp>. Mittags bleibt die Wolkendecke geschlossen und die Höchstwerte liegen bei 1 <temp>. Abends ist es in <city> bedeckt bei Temperaturen von -1 <temp>. Nachts ist es überwiegend dicht bewölkt bei Werten von -1 <temp>. Böen können Geschwindigkeiten zwischen 3 und 18 <velocity> erreichen. Die gefühlten Temperaturen liegen bei -3 bis 2 <temp>.\n",
      "Generated: [PAD] Wetter heute, <date> In <city> kann sich morgens die Sonne nicht durchsetzen und es bleibt bedeckt bei Werten von 5 <temp>. Mittags gibt sich leichte Wolken und das Thermometer klettert auf - <temp>. Am Abend gibt es in <city> lockere Bewölkung bei Werten von 21 bis zu 27 <temp>. Nachts gibt es einen wolkenlosen Himmel bei Tiefstwerten von 23 <temp>. Die Wahrscheinlichkeit für Niederschläge liegt bei 10 <percentile> und es ist mit einer maximalen Niederschlagsmenge\n",
      "\n",
      "Sample 9012:\n",
      "Original: Wetter heute, <date> In <city> ist es morgens leicht bewölkt bei Werten von 18 <temp>. Gegen später ist es sonnig und die Höchstwerte liegen bei 28 <temp>. Am Abend ist es in <city> teils wolkig und teils heiter bei Werten von 19 bis zu 20 <temp>. Nachts bedecken einzelne Wolken den Himmel bei Tiefsttemperaturen von 18 <temp>. Gefühlt liegen die Temperaturen bei 18 bis 30 <temp>.\n",
      "Generated: [PAD] Wetter heute, <date> In <city> regnet es morgens die Sonne bei Höchsttemperaturen bis zu 23 <temp>. Am Abend sollte in <city> der Schirm nicht vergessen werden, da es regnet und die Temperatur liegt bei 22 <temp>. Im weiteren Tagesverlauf sind anhaltende Schauer zu erwarten und das Thermometer klettert auf 37 <temp>. Abends ist es in <city> teils wolkig bei Werten von 20 bis zu 22 <temp>. Nachts gibt es keine Wolken und die Sterne sind klar zu erkennen bei\n",
      "\n",
      "Sample 8024:\n",
      "Original: Wetter heute, <date> In <city> kommt es am Morgen zu einem Mix aus Sonne und Wolken bei Werten von 22 <temp>. Im Laufe des Mittags zeigt sich die Sonne nur vereinzelt bei sonst wolkigem Himmel und das Thermometer klettert auf 38 <temp>. Am Abend ist in <city> der Himmel bedeckt bei Werten von 26 bis zu 30 <temp>. Nachts gibt es einen wolkenlosen Himmel bei Tiefsttemperaturen von 22 <temp>. Die Niederschlagswahrscheinlichkeit liegt bei 15 <percentile>, während mit einer Niederschlagsmenge von maximal 0.04 <rainfall> zu rechnen ist. Die gefühlten Temperaturen liegen bei 23 bis 41 <temp>.\n",
      "Generated: [PAD] Wetter heute, <date> In ist Höchst zeigt regnet es Höchst klar bei 28 <temp>. Am Abend sind in <city> anhaltende Regen Schauer zu erwarten bei Werten von 24 bis zu 22 <temp>. Nachts ist es wolkenlos bei einer Temperatur von 20 <temp>. Mit einer Wahrscheinlichkeit von 15 <percentile>, ist über den Tag verteilt mit Niederschlagsmengen von 0. 01 bis 2. 51 <rainfall> zu rechnen. Gefühlt liegen die Temperaturen bei 23 bis 30 <temp>. <city> liegt in der Region <ne>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_and_run_weather_model(model_path, dataset, sample_indices=None, num_samples=3, tokenizer=None, max_length=100):\n",
    "    \"\"\"\n",
    "    Load the trained GRU weather model and generate text based on features from the provided dataset.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model file\n",
    "        dataset: The cleaned dataset containing features\n",
    "        sample_indices (list, optional): Specific indices to use from the dataset. If None, random samples are selected.\n",
    "        num_samples (int): Number of random samples to generate if sample_indices is None\n",
    "        tokenizer (AutoTokenizer, optional): Tokenizer to use for decoding. If None, will load BERT German tokenizer.\n",
    "        max_length (int): Maximum length of the generated sequence\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping sample indices to original and generated text\n",
    "    \"\"\"\n",
    "    # Load tokenizer if not provided\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
    "        # Add special tokens that were used during training\n",
    "        special_tokens = {\n",
    "            'additional_special_tokens': ['<city>','<temp>','<date>','<velocity>','<percentile>','<rainfall>', '<ne>']\n",
    "        }\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model and metadata\n",
    "    save_data = torch.load(model_path, map_location=device)\n",
    "    token_mappings = save_data['token_mappings']\n",
    "    config = save_data['config']\n",
    "    \n",
    "    # Create model with saved configuration\n",
    "    \n",
    "    # Create model with saved configuration\n",
    "    model = WeatherGRU(\n",
    "        feature_dim=config['feature_dim'],\n",
    "        vocab_size=config['vocab_size'],\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        n_layers=config['n_layers'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    model.load_state_dict(save_data['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Select samples from the dataset\n",
    "    if sample_indices is None:\n",
    "        import random\n",
    "        # Choose random indices\n",
    "        sample_indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "    \n",
    "    # Extract features and original texts\n",
    "    samples = [dataset[i] for i in sample_indices]\n",
    "    features = torch.stack([sample['features'] for sample in samples]).to(device)\n",
    "    original_texts = [sample['text'] for sample in samples]\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(\n",
    "            features,\n",
    "            max_length=max_length,\n",
    "            token_mappings=token_mappings\n",
    "        )\n",
    "        \n",
    "        # Convert tokens back to text\n",
    "        results = {}\n",
    "        for idx, (tokens, original) in enumerate(zip(generated_tokens, original_texts)):\n",
    "            # Map tokens back to original vocabulary\n",
    "            original_tokens = [token_mappings['reverse_token_id_map'][t.item()] for t in tokens]\n",
    "            text = tokenizer.decode(original_tokens, skip_special_tokens=False)\n",
    "            # Clean up text (remove extra tokens or artifacts if needed)\n",
    "            text = re.sub(r'\\[CLS\\]|\\[SEP\\]', '', text).strip()\n",
    "            \n",
    "            sample_idx = sample_indices[idx]\n",
    "            results[sample_idx] = {\n",
    "                'original': original,\n",
    "                'generated': text\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "results = load_and_run_weather_model(\"models/best_gru_model.pt\", clean_dataset)\n",
    "for idx, data in results.items():\n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"Original: {data['original']}\")\n",
    "    print(f\"Generated: {data['generated']}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
