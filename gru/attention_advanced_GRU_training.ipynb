{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import types\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import the standardized models\n",
    "# You can choose which model to use by commenting out one of these imports\n",
    "# from weather_gru_models import AdvancedWeatherGRU as WeatherTextGRU\n",
    "from weather_gru_models import AttentionWeatherGRU as WeatherTextGRU\n",
    "\n",
    "# Import the standardized datasets\n",
    "from weather_datasets import SimpleWeatherDataset, validate_and_clean_data_multithreaded\n",
    "\n",
    "def skip_only_model_special_tokens(tokens, tokenizer):\n",
    "    \"\"\"\n",
    "    Filter out model-specific special tokens from a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (torch.Tensor): List of token IDs\n",
    "        tokenizer: The tokenizer used to encode the text\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Filtered tokens without special tokens\n",
    "    \"\"\"\n",
    "    # IDs of tokens to skip\n",
    "    tokens_to_skip = set([\n",
    "        tokenizer.cls_token_id,\n",
    "        tokenizer.sep_token_id,\n",
    "        tokenizer.pad_token_id\n",
    "    ])\n",
    "    \n",
    "    # Filter out only model special tokens\n",
    "    filtered_tokens = [t for t in tokens if t.item() not in tokens_to_skip]\n",
    "    \n",
    "    # Return as tensor\n",
    "    return torch.tensor(filtered_tokens)\n",
    "\n",
    "def create_improved_dataloader(dataset, batch_size, tokenizer):\n",
    "    \"\"\"\n",
    "    Creates a more efficient DataLoader with dynamic sequence length handling.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to load\n",
    "        batch_size: Batch size for the dataloader\n",
    "        tokenizer: Tokenizer for text encoding\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader: Configured data loader with collate function\n",
    "    \"\"\"\n",
    "    def collate_fn(batch_list):\n",
    "        # Extract features and texts\n",
    "        features = torch.stack([item['features'] for item in batch_list])\n",
    "        texts = [item['text'] for item in batch_list]\n",
    "        \n",
    "        # Normalize features within batch for better training stability\n",
    "        features = (features - features.mean(dim=(0, 1), keepdim=True)) / (\n",
    "            features.std(dim=(0, 1), keepdim=True) + 1e-8)\n",
    "        \n",
    "        # Get token IDs with padding\n",
    "        encoded = tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get sequence lengths for potential packed sequences\n",
    "        seq_lengths = (encoded['attention_mask'] == 1).sum(dim=1)\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'text': encoded['input_ids'],\n",
    "            'attention_mask': encoded['attention_mask'],\n",
    "            'seq_lengths': seq_lengths\n",
    "        }\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "def count_model_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in the model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def prepare_for_improved_training(dataset):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for improved training by splitting into train/validation sets.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to split\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_dataset, val_dataset)\n",
    "    \"\"\"\n",
    "    # Split into train/validation\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        indices, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def load_and_validate_data_multithreaded(data_path='data/files_for_chatGPT/2024-12-12/', num_workers=None):\n",
    "    \"\"\"\n",
    "    Load and validate weather data from JSON files using multiple threads.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to directory containing JSON files\n",
    "        num_workers (int, optional): Number of worker threads to use\n",
    "        \n",
    "    Returns:\n",
    "        list: List of loaded data samples\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    import concurrent.futures\n",
    "    import multiprocessing\n",
    "    \n",
    "    # If num_workers is not specified, use CPU count\n",
    "    if num_workers is None:\n",
    "        num_workers = max(1, multiprocessing.cpu_count() - 1)  # Leave one CPU free\n",
    "    \n",
    "    # Check if we're in the right directory, navigate if needed\n",
    "    if not os.path.exists(data_path):\n",
    "        base_paths = ['.', '..', '../..']\n",
    "        for base in base_paths:\n",
    "            test_path = os.path.join(base, data_path)\n",
    "            if os.path.exists(test_path):\n",
    "                data_path = test_path\n",
    "                break\n",
    "    \n",
    "    # List JSON files\n",
    "    files = [f for f in os.listdir(data_path) if f.endswith('.json')]\n",
    "    print(f\"Found {len(files)} JSON files\")\n",
    "    \n",
    "    # Define function to process a single file\n",
    "    def process_file(file):\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # Validate file - we're using SimpleWeatherDataset, so check for required fields\n",
    "            if not {'gpt_rewritten_apokalyptisch_v2', 'city'}.issubset(data.keys()):\n",
    "                return None\n",
    "                \n",
    "            if not isinstance(data['city'], str) or not data['city'].strip():\n",
    "                return None\n",
    "                \n",
    "            if not isinstance(data['gpt_rewritten_apokalyptisch_v2'], str) or not data['gpt_rewritten_apokalyptisch_v2'].strip():\n",
    "                return None\n",
    "            \n",
    "            # File is valid, return with a key\n",
    "            key = (file.split('-')[-1]).split('_')[0]\n",
    "            return (key, data)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    # Process files in parallel\n",
    "    print(f\"Loading files using {num_workers} workers...\")\n",
    "    with tqdm(total=len(files), desc=\"Loading files\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            # Submit all tasks\n",
    "            futures = {executor.submit(process_file, file): file for file in files}\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    key, data = result\n",
    "                    data_dict[key] = data\n",
    "                pbar.update(1)\n",
    "    \n",
    "    print(f\"Loaded {len(data_dict)} weather data points\")\n",
    "    \n",
    "    # Convert to list\n",
    "    return list(data_dict.values())\n",
    "\n",
    "# Combined function to load, create, and clean dataset in parallel\n",
    "def prepare_dataset_multithreaded(num_workers=None):\n",
    "    \"\"\"\n",
    "    Full pipeline to load, create, and clean the dataset using multiple threads.\n",
    "    \n",
    "    Args:\n",
    "        num_workers (int, optional): Number of worker threads to use\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (clean_dataset, train_dataset, val_dataset)\n",
    "    \"\"\"\n",
    "    print(\"Starting multithreaded dataset preparation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    weather_data = load_and_validate_data_multithreaded(num_workers=num_workers)\n",
    "    \n",
    "    # Create dataset using SimpleWeatherDataset instead of the custom WeatherDataset\n",
    "    dataset = SimpleWeatherDataset(weather_data)\n",
    "    \n",
    "    # Clean dataset using the imported function\n",
    "    clean_dataset = validate_and_clean_data_multithreaded(dataset, num_workers=num_workers)[0]\n",
    "    \n",
    "    # Split into train/validation\n",
    "    train_dataset, val_dataset = prepare_for_improved_training(clean_dataset)\n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Dataset preparation completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return clean_dataset, train_dataset, val_dataset\n",
    "\n",
    "# Set random seed\n",
    "set_seed(42)\n",
    "dataset_start = time.time()\n",
    "\n",
    "# Use multithreaded dataset preparation\n",
    "clean_dataset, train_dataset, val_dataset = prepare_dataset_multithreaded()\n",
    "\n",
    "dataset_end = time.time()\n",
    "print(f\"Dataset preparation completed in {dataset_end - dataset_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def weather_collate_fn(batch_list, tokenizer):\n",
    "    \"\"\"\n",
    "    Collate function for weather data batches.\n",
    "    \n",
    "    Args:\n",
    "        batch_list (list): List of batch items\n",
    "        tokenizer: Tokenizer for text encoding\n",
    "        \n",
    "    Returns:\n",
    "        dict: Batch with features, tokens, and attention mask\n",
    "    \"\"\"\n",
    "    # Extract features and texts\n",
    "    features = torch.stack([item['features'] for item in batch_list])\n",
    "    texts = [item['text'] for item in batch_list]\n",
    "    \n",
    "    # Normalize features within batch for better training stability\n",
    "    features = (features - features.mean(dim=(0, 1), keepdim=True)) / (\n",
    "        features.std(dim=(0, 1), keepdim=True) + 1e-8)\n",
    "    \n",
    "    # Tokenize with padding\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,  # Limit sequence length for efficiency\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'features': features,\n",
    "        'text': encoded['input_ids'],\n",
    "        'attention_mask': encoded['attention_mask']\n",
    "    }\n",
    "\n",
    "def reduce_vocabulary(tokenizer, full_dataset, batch_size=64):\n",
    "    \"\"\"\n",
    "    Identify used tokens and create a reduced vocabulary mapping.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: Tokenizer with full vocabulary\n",
    "        full_dataset: Dataset containing texts to analyze\n",
    "        batch_size: Batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (token_mappings, reduced_vocab_size)\n",
    "    \"\"\"\n",
    "    print(\"Analyzing vocabulary usage to reduce model size...\")\n",
    "    \n",
    "    # Count tokens\n",
    "    token_counter = Counter()\n",
    "    \n",
    "    print(\"Analyzing vocabulary usage to reduce model size...\")\n",
    "    token_counter = Counter()\n",
    "    \n",
    "    # Process the entire dataset directly without DataLoader\n",
    "    for idx in tqdm(range(len(full_dataset)), desc=\"Scanning token usage\"):\n",
    "        # Get the raw text directly (handles Subset objects correctly)\n",
    "        if isinstance(full_dataset, Subset):\n",
    "            sample = full_dataset.dataset[full_dataset.indices[idx]]\n",
    "        else:\n",
    "            sample = full_dataset[idx]\n",
    "        \n",
    "        text = sample['text']\n",
    "        \n",
    "        # Tokenize directly\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        token_counter.update(tokens)\n",
    "    \n",
    "    # Always keep special tokens\n",
    "    for special_token in tokenizer.special_tokens_map.values():\n",
    "        if isinstance(special_token, str):\n",
    "            token_id = tokenizer.convert_tokens_to_ids(special_token)\n",
    "            if token_id not in token_counter:\n",
    "                token_counter[token_id] = 1\n",
    "        elif isinstance(special_token, list):\n",
    "            for token in special_token:\n",
    "                token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "                if token_id not in token_counter:\n",
    "                    token_counter[token_id] = 1\n",
    "    \n",
    "    # Sort by frequency for efficient token ID assignment\n",
    "    used_token_ids = sorted(token_counter.keys())\n",
    "    \n",
    "    # Create token ID mapping (old ID -> new ID)\n",
    "    token_id_map = {old_id: new_id for new_id, old_id in enumerate(used_token_ids)}\n",
    "    \n",
    "    # Create reverse mapping for inference\n",
    "    reverse_token_id_map = {new_id: old_id for old_id, new_id in token_id_map.items()}\n",
    "    \n",
    "    # Store the mappings for later use\n",
    "    token_mappings = {\n",
    "        'token_id_map': token_id_map,\n",
    "        'reverse_token_id_map': reverse_token_id_map,\n",
    "        'used_token_ids': used_token_ids\n",
    "    }\n",
    "    \n",
    "    # Update vocabulary size to reduced size\n",
    "    reduced_vocab_size = len(used_token_ids)\n",
    "    original_vocab_size = len(tokenizer.vocab)\n",
    "    print(f\"Reduced vocabulary from {original_vocab_size:,} to {reduced_vocab_size:,} tokens \" \n",
    "          f\"({reduced_vocab_size/original_vocab_size*100:.1f}%)\")\n",
    "    \n",
    "    return token_mappings, reduced_vocab_size\n",
    "\n",
    "# Map tokens function for data loaders\n",
    "def map_tokens_fn(batch, token_id_map):\n",
    "    \"\"\"\n",
    "    Map token IDs from original vocabulary to reduced vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        batch: Batch containing token IDs\n",
    "        token_id_map: Mapping from original to reduced token IDs\n",
    "        \n",
    "    Returns:\n",
    "        dict: Batch with mapped token IDs\n",
    "    \"\"\"\n",
    "    # Map the token IDs to new IDs\n",
    "    old_tokens = batch['text']\n",
    "    new_tokens = torch.zeros_like(old_tokens)\n",
    "    \n",
    "    # Apply mapping\n",
    "    for i in range(old_tokens.size(0)):\n",
    "        for j in range(old_tokens.size(1)):\n",
    "            old_id = old_tokens[i, j].item()\n",
    "            new_tokens[i, j] = token_id_map.get(old_id, 0)  # Default to 0 if token not found\n",
    "    \n",
    "    batch['text'] = new_tokens\n",
    "    return batch\n",
    "\n",
    "def train_model(args):\n",
    "    \"\"\"\n",
    "    Train the weather text generation model.\n",
    "    \n",
    "    Args:\n",
    "        args: Arguments for training configuration\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer, token_mappings)\n",
    "    \"\"\"\n",
    "    global tokenizer  # Make tokenizer accessible to model\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "    # Load tokenizer - using German BERT for German text\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "    \n",
    "    # Add special tokens that appear in our texts\n",
    "    special_tokens = {'additional_special_tokens': ['<city>', '<temp>']}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    print(\"Preparing datasets...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Dataset preparation completed in {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "    # Reduce vocabulary\n",
    "    token_mappings, reduced_vocab_size = reduce_vocabulary(tokenizer, clean_dataset, args.batch_size)\n",
    "    \n",
    "    # Create dataloaders with token mapping\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: map_tokens_fn(weather_collate_fn(batch, tokenizer), token_mappings['token_id_map'])\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: map_tokens_fn(weather_collate_fn(batch, tokenizer), token_mappings['token_id_map'])\n",
    "    )\n",
    "    \n",
    "    # Get feature dimension\n",
    "    feature_dim = clean_dataset.dataset.feature_dim\n",
    "    \n",
    "    print(f\"Feature dimension: {feature_dim}\")\n",
    "    print(f\"Reduced vocabulary size: {reduced_vocab_size}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    model = WeatherTextGRU(\n",
    "        feature_dim=feature_dim,\n",
    "        hidden_size=args.hidden_size,\n",
    "        vocab_size=reduced_vocab_size,\n",
    "        dropout=args.dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {num_params:,} trainable parameters\")\n",
    "    \n",
    "    # Ensure we're under the 3M parameter limit\n",
    "    if num_params > 30_000_000:\n",
    "        print(f\"WARNING: Model exceeds 3M parameters ({num_params:,}), reducing hidden size...\")\n",
    "        \n",
    "        # Reduce hidden size until we're under 3M params\n",
    "        while num_params > 30_000_000 and args.hidden_size > 128:\n",
    "            args.hidden_size -= 32\n",
    "            \n",
    "            model = WeatherTextGRU(\n",
    "                feature_dim=feature_dim,\n",
    "                hidden_size=args.hidden_size,\n",
    "                vocab_size=reduced_vocab_size,\n",
    "                dropout=args.dropout\n",
    "            ).to(device)\n",
    "            \n",
    "            num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            print(f\"Adjusted model: {num_params:,} parameters with hidden_size={args.hidden_size}\")\n",
    "    \n",
    "    # Setup aggressive optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Loss function (with 'none' reduction for masking)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    # Or try focal loss to focus on harder examples\n",
    "    def focal_loss(predictions, targets, gamma=2.0, alpha=0.25):\n",
    "        ce_loss = F.cross_entropy(predictions, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = alpha * (1-pt)**gamma * ce_loss\n",
    "        return focal_loss\n",
    "\n",
    "    # Learning rate scheduler for aggressive learning\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=args.lr * 10,  # Peak at 10x the base learning rate\n",
    "        total_steps=args.epochs * len(train_loader),\n",
    "        pct_start=0.1,  # Aggressive warm-up\n",
    "        div_factor=25.0,  # Determines initial lr\n",
    "        final_div_factor=10000.0,  # For steep decay at the end\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    # Training parameters\n",
    "    best_val_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    clip_value = 1.0\n",
    "    \n",
    "    # Prepare save directory\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    \n",
    "    training_start = time.time()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for batch in train_pbar:\n",
    "            # Move data to device\n",
    "            features = batch['features'].to(device)\n",
    "            tokens = batch['text'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass with very aggressive teacher forcing\n",
    "            outputs = model(features, tokens, teacher_forcing_ratio=0.8)\n",
    "            \n",
    "            # Calculate loss (excluding first token - CLS)\n",
    "            output_flat = outputs[:, 1:].reshape(-1, outputs.shape[-1])\n",
    "            target_flat = tokens[:, 1:].reshape(-1)\n",
    "            mask_flat = attention_mask[:, 1:].reshape(-1)\n",
    "            \n",
    "            # Compute token-wise loss\n",
    "            losses = criterion(output_flat, target_flat)\n",
    "            \n",
    "            # Apply mask to ignore padding tokens\n",
    "            masked_losses = losses * mask_flat\n",
    "            \n",
    "            # Average over non-padding tokens\n",
    "            batch_loss = masked_losses.sum() / (mask_flat.sum() + 1e-8)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss += batch_loss.item()\n",
    "            train_pbar.set_postfix({\"loss\": f\"{batch_loss.item():.4f}\"})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=\"Validating\")\n",
    "            for batch in val_pbar:\n",
    "                features = batch['features'].to(device)\n",
    "                tokens = batch['text'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Forward pass (no teacher forcing)\n",
    "                outputs = model(features, tokens, teacher_forcing_ratio=0.0)\n",
    "                \n",
    "                # Calculate loss\n",
    "                output_flat = outputs[:, 1:].reshape(-1, outputs.shape[-1])\n",
    "                target_flat = tokens[:, 1:].reshape(-1)\n",
    "                mask_flat = attention_mask[:, 1:].reshape(-1)\n",
    "                \n",
    "                losses = criterion(output_flat, target_flat)\n",
    "                masked_losses = losses * mask_flat\n",
    "                batch_loss = masked_losses.sum() / (mask_flat.sum() + 1e-8)\n",
    "                \n",
    "                val_loss += batch_loss.item()\n",
    "                val_pbar.set_postfix({\"loss\": f\"{batch_loss.item():.4f}\"})\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        # After computing train_loss for each epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)  \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1} completed in {epoch_time:.2f}s\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "        # Save model and token mappings\n",
    "        if avg_train_loss < best_train_loss:\n",
    "            best_train_loss = avg_train_loss\n",
    "            # Save model and token mappings\n",
    "            model_path = os.path.join(args.save_dir, \"best_weather_text_model.pt\")\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'token_mappings': token_mappings,\n",
    "                'model_config': {\n",
    "                    'feature_dim': feature_dim,\n",
    "                    'hidden_size': args.hidden_size,\n",
    "                    'vocab_size': reduced_vocab_size,\n",
    "                    'dropout': args.dropout\n",
    "                },\n",
    "                'train_args': vars(args),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss\n",
    "            }, model_path)\n",
    "            \n",
    "            print(f\"✓ Model saved to {model_path}!\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= args.patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Generate sample text periodically\n",
    "        if (epoch + 1) % 3 == 0 or epoch == 0 or epoch == args.epochs - 1:\n",
    "            generate_samples(model, tokenizer, val_loader, token_mappings, 1)\n",
    "    \n",
    "    total_time = time.time() - training_start\n",
    "    print(f\"\\nTraining completed in {total_time:.2f}s\")\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    checkpoint = torch.load(os.path.join(args.save_dir, \"best_weather_text_model.pt\"))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    final_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Final Evaluation\"):\n",
    "            features = batch['features'].to(device)\n",
    "            tokens = batch['text'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(features, tokens, teacher_forcing_ratio=0.0)\n",
    "            \n",
    "            output_flat = outputs[:, 1:].reshape(-1, outputs.shape[-1])\n",
    "            target_flat = tokens[:, 1:].reshape(-1)\n",
    "            mask_flat = attention_mask[:, 1:].reshape(-1)\n",
    "            \n",
    "            losses = criterion(output_flat, target_flat)\n",
    "            masked_losses = losses * mask_flat\n",
    "            batch_loss = masked_losses.sum() / (mask_flat.sum() + 1e-8)\n",
    "            \n",
    "            final_val_loss += batch_loss.item()\n",
    "    \n",
    "    final_val_loss /= len(val_loader)\n",
    "    print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
    "    \n",
    "    # Generate final samples\n",
    "    print(\"\\nFinal generated samples:\")\n",
    "    generate_samples(model, tokenizer, val_loader, token_mappings, args.sample_count)\n",
    "    \n",
    "    return model, tokenizer, token_mappings\n",
    "\n",
    "def generate_samples(model, tokenizer, data_loader, token_mappings, num_samples=5):\n",
    "    \"\"\"\n",
    "    Generate text samples from the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        tokenizer: Tokenizer for decoding\n",
    "        data_loader: DataLoader for getting example features\n",
    "        token_mappings: Mapping between original and reduced vocabulary\n",
    "        num_samples: Number of samples to generate\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get mapping for converting back to original token IDs\n",
    "    reverse_map = token_mappings['reverse_token_id_map']\n",
    "    \n",
    "    # IDs of tokens to keep (exclude [PAD], [CLS], [SEP])\n",
    "    tokens_to_exclude = {\n",
    "        tokenizer.pad_token_id,\n",
    "        tokenizer.cls_token_id, \n",
    "        tokenizer.sep_token_id\n",
    "    }\n",
    "\n",
    "    # Get samples\n",
    "    samples = []\n",
    "    data_iter = iter(data_loader)\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        try:\n",
    "            sample_batch = next(data_iter)\n",
    "            samples.append(sample_batch)\n",
    "        except StopIteration:\n",
    "            # Reset iterator if we run out of samples\n",
    "            data_iter = iter(data_loader)\n",
    "            sample_batch = next(data_iter)\n",
    "            samples.append(sample_batch)\n",
    "    \n",
    "    # Generate text for each sample\n",
    "    for i, batch in enumerate(samples):\n",
    "        sample_idx = 0  # Just use the first item in the batch\n",
    "        \n",
    "        # Get features and original tokens\n",
    "        sample_features = batch['features'][sample_idx].unsqueeze(0).to(device)\n",
    "        sample_tokens_mapped = batch['text'][sample_idx]\n",
    "        \n",
    "        # Get temperature, humidity and cloudiness data for reference\n",
    "        temp_values = sample_features[0, :, 0].cpu().numpy()  # Temperature (first feature)\n",
    "        humidity_values = sample_features[0, :, 1].cpu().numpy()  # Humidity (second feature)\n",
    "        cloud_values = sample_features[0, :, 2].cpu().numpy()  # Cloudiness (third feature)\n",
    "        \n",
    "        # Generate text\n",
    "        with torch.no_grad():\n",
    "            generated_output = model(sample_features)\n",
    "        \n",
    "        # Get token predictions\n",
    "        generated_tokens_mapped = generated_output[0].argmax(dim=1)\n",
    "        \n",
    "        # Map tokens back to original vocabulary\n",
    "        original_tokens = torch.tensor([\n",
    "            reverse_map[token.item()] for token in sample_tokens_mapped\n",
    "        ])\n",
    "        \n",
    "        generated_tokens = torch.tensor([\n",
    "            reverse_map[token.item()] for token in generated_tokens_mapped\n",
    "        ])\n",
    "        \n",
    "        # Filter out unwanted special tokens from generation\n",
    "        filtered_generated = [token.item() for token in generated_tokens \n",
    "                           if token.item() not in tokens_to_exclude]\n",
    "        \n",
    "        # Decode to text\n",
    "        original_text = tokenizer.decode(original_tokens, skip_special_tokens=False)\n",
    "        generated_text = tokenizer.decode(filtered_generated, skip_special_tokens=False)\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Temperature: {[round(float(t), 1) for t in temp_values]}\")\n",
    "        print(f\"Humidity: {[round(float(h), 1) for h in humidity_values]}\")\n",
    "        print(f\"Cloudiness: {[round(float(c), 2) for c in cloud_values]}\")\n",
    "        print(f\"Original: {original_text}\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def generate_weather_text(model, tokenizer, features, token_mappings):\n",
    "    \"\"\"\n",
    "    Generate a weather text description from features.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained weather model\n",
    "        tokenizer: Tokenizer for decoding\n",
    "        features: Weather features tensor\n",
    "        token_mappings: Mapping between original and reduced vocabulary\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated weather text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # IDs of tokens to exclude\n",
    "    tokens_to_exclude = {\n",
    "        tokenizer.pad_token_id,\n",
    "        tokenizer.cls_token_id, \n",
    "        tokenizer.sep_token_id\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Normalize features\n",
    "        features = (features - features.mean(dim=0, keepdim=True)) / (\n",
    "            features.std(dim=0, keepdim=True) + 1e-8)\n",
    "        \n",
    "        # Add batch dimension and move to device\n",
    "        features = features.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate text\n",
    "        outputs = model(features)\n",
    "        \n",
    "        # Using temperature sampling for more diverse outputs\n",
    "        temperature = 1.0\n",
    "        probs = F.softmax(outputs[0] / temperature, dim=1)\n",
    "        generated_tokens_mapped = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "        \n",
    "        # Map tokens back to original vocabulary\n",
    "        reverse_map = token_mappings['reverse_token_id_map']\n",
    "        generated_tokens = [reverse_map[token.item()] for token in generated_tokens_mapped]\n",
    "        \n",
    "        # Filter out unwanted special tokens\n",
    "        filtered_tokens = [token for token in generated_tokens \n",
    "                          if token not in tokens_to_exclude]\n",
    "        \n",
    "        # Start text with \"In\" if it doesn't already\n",
    "        if filtered_tokens and tokenizer.decode([filtered_tokens[0]]) != \"In\":\n",
    "            in_token_id = tokenizer.convert_tokens_to_ids(\"In\")\n",
    "            filtered_tokens = [in_token_id] + filtered_tokens\n",
    "        \n",
    "        # Decode to text\n",
    "        generated_text = tokenizer.decode(filtered_tokens, skip_special_tokens=False)\n",
    "        \n",
    "        return generated_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create arguments object manually for notebook environment\n",
    "    from types import SimpleNamespace\n",
    "    args = SimpleNamespace(\n",
    "        epochs=10,\n",
    "        batch_size=128,\n",
    "        hidden_size=512,\n",
    "        lr=1e-3,\n",
    "        dropout=0.3,\n",
    "        patience=6,\n",
    "        save_dir='./models',\n",
    "        seed=42,\n",
    "        test_only=False,\n",
    "        sample_count=5,\n",
    "        model_path=None\n",
    "    )\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    \n",
    "    # Train or load model\n",
    "    if not args.test_only:\n",
    "        print(f\"Training model with {args.epochs} epochs, batch size {args.batch_size}, hidden size {args.hidden_size}\")\n",
    "        model, tokenizer, token_mappings = train_model(args)\n",
    "    else:\n",
    "        # Load model for testing\n",
    "        print(\"Loading pre-trained model for inference...\")\n",
    "        \n",
    "        # Determine model path\n",
    "        model_path = args.model_path or os.path.join(args.save_dir, \"best_weather_text_model.pt\")\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Error: Model file not found at {model_path}\")\n",
    "            exit(1)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "        special_tokens = {'additional_special_tokens': ['<city>', '<temp>']}\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        \n",
    "        # Get token mappings\n",
    "        token_mappings = checkpoint['token_mappings']\n",
    "        \n",
    "        # Get model config\n",
    "        model_config = checkpoint['model_config']\n",
    "        \n",
    "        # Create model\n",
    "        model = WeatherTextGRU(\n",
    "            feature_dim=model_config['feature_dim'],\n",
    "            hidden_size=model_config['hidden_size'],\n",
    "            vocab_size=model_config['vocab_size'],\n",
    "            dropout=model_config.get('dropout', 0.2)\n",
    "        ).to(device)\n",
    "        \n",
    "        # Load weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        print(f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters\")\n",
    "        \n",
    "        # Prepare datasets for testing\n",
    "        set_seed(args.seed)\n",
    "        \n",
    "        # Create dataloader with token mapping\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda batch: map_tokens_fn(\n",
    "                weather_collate_fn(batch, tokenizer), \n",
    "                token_mappings['token_id_map']\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Generate samples\n",
    "        print(f\"\\nGenerating {args.sample_count} weather text samples:\")\n",
    "        generate_samples(model, tokenizer, val_loader, token_mappings, args.sample_count)\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
