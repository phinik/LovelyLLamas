{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoTokenizer\n",
    "from weather_gru_models import BasicWeatherGRU\n",
    "from weather_datasets import StandardWeatherDataset, prepare_weather_dataset, validate_and_clean_data_multithreaded, reduce_vocabulary\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "# Special tokens for text substitution\n",
    "special_tokens = {\n",
    "    'additional_special_tokens': ['<city>','<temp>','<date>','<velocity>','<percentile>','<rainfall>', '<ne>']\n",
    "}\n",
    "\n",
    "# Add special tokens into tokenizer\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    # change directory if not on root\n",
    "    if str(os.getcwd()).endswith('LLamas') == False:\n",
    "        os.chdir('..')\n",
    "    \n",
    "    # Define file validation function\n",
    "    def check_file(path):\n",
    "        \"\"\"\n",
    "        Checks if the file contains valid keys and values for StandardWeatherDataset.\n",
    "        Returns True if the file should be loaded, False otherwise.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to the JSON file\n",
    "            \n",
    "        Returns:\n",
    "            bool: Whether file is valid\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data: dict = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Invalid JSON in file: {path}\")\n",
    "                return False\n",
    "\n",
    "            # Ensure required keys are present\n",
    "            if not {'report_long', 'city'}.issubset(data.keys()):\n",
    "                return False\n",
    "\n",
    "            # Check if 'city' and 'report_long' have valid non-empty values\n",
    "            if not isinstance(data['city'], str) or not data['city'].strip():\n",
    "                return False\n",
    "            if not isinstance(data['report_long'], str) or not data['report_long'].strip():\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "                \n",
    "    # Load data from file\n",
    "    def load_data(path):\n",
    "        \"\"\"\n",
    "        Load JSON data from file path.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to the JSON file\n",
    "            \n",
    "        Returns:\n",
    "            dict: Loaded data dictionary\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data: dict = json.load(f)\n",
    "            return data\n",
    "    \n",
    "    # List files for reading\n",
    "    files = os.listdir(os.path.join(os.getcwd(), 'data', 'files_for_chatGPT', '2024-12-12'))\n",
    "    \n",
    "    # Load files with validation\n",
    "    files = {(file.split('-')[-1]).split('_')[0]:load_data(os.path.join(os.getcwd(), 'data', 'files_for_chatGPT', '2024-12-12', file)) \n",
    "             for file in tqdm(files) \n",
    "             if check_file(os.path.join(os.getcwd(), 'data', 'files_for_chatGPT', '2024-12-12', file))}\n",
    "    \n",
    "    # Convert dictionary values to list for dataset creation\n",
    "    weather_data = list(files.values())\n",
    "    \n",
    "    # Create StandardWeatherDataset\n",
    "    dataset = StandardWeatherDataset(weather_data, max_length=100)\n",
    "    \n",
    "    # Scan dataset for named entities (optional)\n",
    "    entity_counts = dataset.scan_dataset_for_named_entities(sample_size=1000)\n",
    "    print(entity_counts)\n",
    "    \n",
    "    # Clean the dataset using the standardized validation function\n",
    "    clean_dataset, valid_indices = validate_and_clean_data_multithreaded(dataset, 11)\n",
    "\n",
    "    # Output validation summary\n",
    "    print(f\"\\nDataset validation complete. Kept {len(valid_indices)} valid samples out of {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): PyTorch model\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of trainable parameters\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def adjust_model_size(feature_dim, vocab_size, embedding_dim, hidden_dim, max_params=30_000_000):\n",
    "    \"\"\"\n",
    "    Adjust model dimensions to stay under parameter limit.\n",
    "    \n",
    "    Args:\n",
    "        feature_dim (int): Dimension of input features\n",
    "        vocab_size (int): Size of vocabulary\n",
    "        embedding_dim (int): Initial embedding dimension\n",
    "        hidden_dim (int): Initial hidden dimension\n",
    "        max_params (int): Maximum parameter limit\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (adjusted_embedding_dim, adjusted_hidden_dim)\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    # Calculate WeatherGRU model parameter count\n",
    "    # Main parameters:\n",
    "    # - Feature encoder: feature_dim * embedding_dim * 2 (two linear layers)\n",
    "    # - Embedding: vocab_size * embedding_dim\n",
    "    # - GRU: embedding_dim * hidden_dim * 3 (input projection) + hidden_dim * hidden_dim * 3 (recurrent)\n",
    "    # - Output layer: hidden_dim * vocab_size\n",
    "    \n",
    "    param_count = (\n",
    "        feature_dim * embedding_dim * 2 +  # Feature encoder (two layers)\n",
    "        vocab_size * embedding_dim +       # Embedding layer\n",
    "        (embedding_dim * hidden_dim * 3) + # GRU input projection (3 gates)\n",
    "        (hidden_dim * hidden_dim * 3) +    # GRU recurrent connections (3 gates)\n",
    "        hidden_dim * vocab_size            # Output layer\n",
    "    )\n",
    "    \n",
    "    print(f\"Initial parameter count: {param_count:,}\")\n",
    "    \n",
    "    if param_count <= max_params:\n",
    "        return embedding_dim, hidden_dim\n",
    "    \n",
    "    # If we need to reduce params, try to find optimal dimensions\n",
    "    # Strategy: reduce both embedding_dim and hidden_dim proportionally\n",
    "    \n",
    "    # Calculate reduction factor needed\n",
    "    reduction_factor = math.sqrt(max_params / param_count)\n",
    "    \n",
    "    # Apply reduction\n",
    "    new_embedding_dim = max(32, int(embedding_dim * reduction_factor))\n",
    "    new_hidden_dim = max(64, int(hidden_dim * reduction_factor))\n",
    "    \n",
    "    # Recalculate to verify\n",
    "    new_param_count = (\n",
    "        feature_dim * new_embedding_dim * 2 +  \n",
    "        vocab_size * new_embedding_dim +       \n",
    "        (new_embedding_dim * new_hidden_dim * 3) + \n",
    "        (new_hidden_dim * new_hidden_dim * 3) +    \n",
    "        new_hidden_dim * vocab_size            \n",
    "    )\n",
    "    \n",
    "    print(f\"Adjusted dimensions: embedding_dim={new_embedding_dim}, hidden_dim={new_hidden_dim}\")\n",
    "    print(f\"Adjusted parameter count: {new_param_count:,}\")\n",
    "    \n",
    "    return new_embedding_dim, new_hidden_dim\n",
    "\n",
    "def save_model_with_metadata(model, path, token_mappings=None, config=None):\n",
    "    \"\"\"\n",
    "    Save model with all required metadata for later use.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Model to save\n",
    "        path (str): Path to save model\n",
    "        token_mappings (dict, optional): Token ID mappings for reduced vocabulary\n",
    "        config (dict, optional): Model configuration parameters\n",
    "        \n",
    "    Returns:\n",
    "        str: Path where model was saved\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import os\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Prepare save data\n",
    "    save_data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # Add token mappings if available\n",
    "    if token_mappings is not None:\n",
    "        save_data['token_mappings'] = token_mappings\n",
    "    \n",
    "    # Add model configuration if available\n",
    "    if config is not None:\n",
    "        save_data['config'] = config\n",
    "    \n",
    "    # Save to file\n",
    "    torch.save(save_data, path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "    \n",
    "    return path\n",
    "\n",
    "def load_model_with_metadata(path, device='cpu'):\n",
    "    \"\"\"\n",
    "    Load model with all metadata.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the saved model file\n",
    "        device (str): Device to load model to\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing model state_dict, token_mappings, and config\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    # Load saved data\n",
    "    save_data = torch.load(path, map_location=device)\n",
    "    \n",
    "    return save_data\n",
    "\n",
    "def create_dataloader_with_mapping(dataset, batch_size, tokenizer, token_id_map=None):\n",
    "    \"\"\"\n",
    "    Create a DataLoader with optional token mapping for reduced vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Weather dataset\n",
    "        batch_size (int): Batch size\n",
    "        tokenizer: Tokenizer for text encoding\n",
    "        token_id_map (dict, optional): Mapping for token IDs in reduced vocabulary\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader: Configured data loader\n",
    "    \"\"\"\n",
    "    def prepare_batch_with_mapping(batch_list, tokenizer, token_id_map=None):\n",
    "        \"\"\"\n",
    "        Prepare batch with optional token mapping for reduced vocabulary.\n",
    "        \n",
    "        Args:\n",
    "            batch_list (list): List of sample dictionaries\n",
    "            tokenizer: Tokenizer for text encoding\n",
    "            token_id_map (dict, optional): Token ID mapping for reduced vocabulary\n",
    "            \n",
    "        Returns:\n",
    "            dict: Prepared batch with features and text tokens\n",
    "        \"\"\"\n",
    "        features = torch.stack([item['features'] for item in batch_list])\n",
    "        texts = [item['text'] for item in batch_list]\n",
    "        \n",
    "        # Normalize features\n",
    "        features = (features - features.mean()) / (features.std() + 1e-8)\n",
    "        \n",
    "        encoded = tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        batch = {\n",
    "            'features': features,\n",
    "            'text': encoded['input_ids']\n",
    "        }\n",
    "        \n",
    "        # Apply token mapping if provided\n",
    "        if token_id_map is not None:\n",
    "            # Map the token IDs to new IDs\n",
    "            old_tokens = batch['text']\n",
    "            new_tokens = torch.zeros_like(old_tokens)\n",
    "            \n",
    "            for i in range(old_tokens.size(0)):\n",
    "                for j in range(old_tokens.size(1)):\n",
    "                    old_id = old_tokens[i, j].item()\n",
    "                    new_tokens[i, j] = token_id_map.get(old_id, 0)  # Default to 0 if token not found\n",
    "            \n",
    "            batch['text'] = new_tokens\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: prepare_batch_with_mapping(batch, tokenizer, token_id_map)\n",
    "    )\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, teacher_forcing_ratio=1.0, epoch=0, total_epochs=1):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Model to train\n",
    "        dataloader (DataLoader): DataLoader with training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        teacher_forcing_ratio (float): Probability of using teacher forcing\n",
    "        epoch (int): Current epoch number\n",
    "        total_epochs (int): Total number of epochs\n",
    "        \n",
    "    Returns:\n",
    "        float: Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        try:\n",
    "            features = batch['features'].to(device)\n",
    "            text = batch['text'].to(device)\n",
    "            \n",
    "            # Check for invalid values in inputs\n",
    "            if torch.isnan(features).any() or torch.isinf(features).any():\n",
    "                print(f\"Warning: Invalid values in features at batch {batch_idx}\")\n",
    "                continue\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with gradient checking\n",
    "            with torch.autograd.detect_anomaly():\n",
    "                outputs = model(features, text, teacher_forcing_ratio)\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                targets = text[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Check if loss is valid\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"Warning: Invalid loss value {loss.item()} at batch {batch_idx}\")\n",
    "                    print(\"Last output values:\", outputs[-5:])\n",
    "                    print(\"Last target values:\", targets[-5:])\n",
    "                    raise ValueError(\"Invalid loss detected\")\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Check gradients\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        grad_norm = param.grad.norm()\n",
    "                        if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n",
    "                            print(f\"Warning: Invalid gradient for {name}\")\n",
    "                            raise ValueError(f\"Invalid gradient detected in {name}\")\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                avg_loss = total_loss / num_batches\n",
    "                pbar.set_postfix({\"Loss\": f\"{avg_loss:.4f}\"})\n",
    "                \n",
    "        except ValueError as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "\n",
    "def train_model(dataset, tokenizer, max_params=3_000_000, num_epochs=10, batch_size=64, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Train a GRU model on the weather dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Weather dataset\n",
    "        tokenizer: Tokenizer for text encoding\n",
    "        max_params (int): Maximum number of parameters for the model\n",
    "        num_epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "        learning_rate (float): Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained_model, token_mappings, training_losses)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # First, reduce vocabulary\n",
    "    token_mappings, reduced_vocab_size = reduce_vocabulary(tokenizer, dataset, batch_size)\n",
    "    \n",
    "    # Get feature dimension\n",
    "    feature_dim = dataset.feature_dim\n",
    "    \n",
    "    # Determine optimal model dimensions based on parameter constraints\n",
    "    embedding_dim, hidden_dim = adjust_model_size(\n",
    "        feature_dim=feature_dim,\n",
    "        vocab_size=reduced_vocab_size,\n",
    "        embedding_dim=256,  # Starting point\n",
    "        hidden_dim=512,     # Starting point\n",
    "        max_params=max_params\n",
    "    )\n",
    "    \n",
    "    # Create model with adjusted dimensions\n",
    "    model = BasicWeatherGRU(\n",
    "        feature_dim=feature_dim,\n",
    "        vocab_size=reduced_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_layers=1,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    param_count = count_model_parameters(model)\n",
    "    print(f\"Model has {param_count:,} trainable parameters\")\n",
    "    \n",
    "    # Create dataloader with token mapping\n",
    "    dataloader = create_dataloader_with_mapping(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        tokenizer=tokenizer,\n",
    "        token_id_map=token_mappings['token_id_map']\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='mean')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Add learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Select a few examples for generation testing\n",
    "    test_indices = [0, len(dataset)//2, len(dataset)-1]  # Beginning, middle, and end\n",
    "    test_samples = [dataset[i] for i in test_indices]\n",
    "    test_features = torch.stack([sample['features'] for sample in test_samples]).to(device)\n",
    "    \n",
    "    print(\"\\nOriginal texts for test samples:\")\n",
    "    for idx, sample in zip(test_indices, test_samples):\n",
    "        print(f\"Sample {idx}: {sample['text']}\")\n",
    "    \n",
    "    # Create directory for model checkpoints\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        try:\n",
    "            loss = train_epoch(\n",
    "                model, dataloader, criterion, optimizer, device,\n",
    "                teacher_forcing_ratio=0.9,\n",
    "                epoch=epoch, total_epochs=num_epochs\n",
    "            )\n",
    "            losses.append(loss)\n",
    "            \n",
    "            scheduler.step(loss)\n",
    "            print(f\"\\nEpoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            # Generate examples\n",
    "            print(\"\\nGenerated examples:\")\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = model.generate(\n",
    "                    test_features,\n",
    "                    token_mappings=token_mappings\n",
    "                )\n",
    "                for idx, tokens in enumerate(generated_tokens):\n",
    "                    # Map tokens back to original vocabulary\n",
    "                    original_tokens = [token_mappings['reverse_token_id_map'][t.item()] for t in tokens]\n",
    "                    generated_text = tokenizer.decode(original_tokens, skip_special_tokens=False)\n",
    "                    print(f\"Sample {test_indices[idx]}: {generated_text}\")\n",
    "            model.train()\n",
    "            \n",
    "            # Save checkpoint if it's the best model so far\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                config = {\n",
    "                    'feature_dim': feature_dim,\n",
    "                    'vocab_size': reduced_vocab_size,\n",
    "                    'embedding_dim': embedding_dim,\n",
    "                    'hidden_dim': hidden_dim,\n",
    "                    'n_layers': 1,\n",
    "                    'dropout': 0.2,\n",
    "                    'epoch': epoch,\n",
    "                    'loss': loss\n",
    "                }\n",
    "                \n",
    "                save_model_with_metadata(\n",
    "                    model=model,\n",
    "                    path=os.path.join('models', 'best_gru_model.pt'),\n",
    "                    token_mappings=token_mappings,\n",
    "                    config=config\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during epoch {epoch + 1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return model, token_mappings, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "# Note that we now call train_model differently, passing dataset and tokenizer directly\n",
    "model, token_mappings, losses = train_model(\n",
    "    dataset=clean_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_params=30_000_000,  # 30 million parameter limit\n",
    "    num_epochs=1,           # tends to overfit after 1 epoch\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_run_weather_model(model_path, dataset, sample_indices=None, num_samples=3, tokenizer=None, max_length=100):\n",
    "    \"\"\"\n",
    "    Load the trained GRU weather model and generate text based on features from the provided dataset.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model file\n",
    "        dataset: The cleaned dataset containing features\n",
    "        sample_indices (list, optional): Specific indices to use from the dataset. If None, random samples are selected.\n",
    "        num_samples (int): Number of random samples to generate if sample_indices is None\n",
    "        tokenizer (AutoTokenizer, optional): Tokenizer to use for decoding. If None, will load BERT German tokenizer.\n",
    "        max_length (int): Maximum length of the generated sequence\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping sample indices to original and generated text\n",
    "    \"\"\"\n",
    "    # Load tokenizer if not provided\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
    "        # Add special tokens that were used during training\n",
    "        special_tokens = {\n",
    "            'additional_special_tokens': ['<city>','<temp>','<date>','<velocity>','<percentile>','<rainfall>', '<ne>']\n",
    "        }\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model and metadata\n",
    "    save_data = torch.load(model_path, map_location=device)\n",
    "    token_mappings = save_data['token_mappings']\n",
    "    config = save_data['config']\n",
    "    \n",
    "    # Create model with saved configuration\n",
    "    model = BasicWeatherGRU(\n",
    "        feature_dim=config['feature_dim'],\n",
    "        vocab_size=config['vocab_size'],\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        n_layers=config['n_layers'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    model.load_state_dict(save_data['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Select samples from the dataset\n",
    "    if sample_indices is None:\n",
    "        import random\n",
    "        # Choose random indices\n",
    "        sample_indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "    \n",
    "    # Extract features and original texts\n",
    "    samples = [dataset[i] for i in sample_indices]\n",
    "    features = torch.stack([sample['features'] for sample in samples]).to(device)\n",
    "    original_texts = [sample['text'] for sample in samples]\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(\n",
    "            features,\n",
    "            max_length=max_length,\n",
    "            token_mappings=token_mappings\n",
    "        )\n",
    "        \n",
    "        # Convert tokens back to text\n",
    "        results = {}\n",
    "        for idx, (tokens, original) in enumerate(zip(generated_tokens, original_texts)):\n",
    "            # Map tokens back to original vocabulary\n",
    "            original_tokens = [token_mappings['reverse_token_id_map'][t.item()] for t in tokens]\n",
    "            text = tokenizer.decode(original_tokens, skip_special_tokens=False)\n",
    "            # Clean up text (remove extra tokens or artifacts if needed)\n",
    "            text = re.sub(r'\\[CLS\\]|\\[SEP\\]', '', text).strip()\n",
    "            \n",
    "            sample_idx = sample_indices[idx]\n",
    "            results[sample_idx] = {\n",
    "                'original': original,\n",
    "                'generated': text\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = load_and_run_weather_model(\"models/best_gru_model.pt\", clean_dataset)\n",
    "# for idx, data in results.items():\n",
    "#     print(f\"Sample {idx}:\")\n",
    "#     print(f\"Original: {data['original']}\")\n",
    "#     print(f\"Generated: {data['generated']}\")\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
